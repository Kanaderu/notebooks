{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation\n",
    "The expected value for a random variable $X$ is given by\n",
    "$$E[X]=\\int_{-\\infty}^{\\infty}xf_X(x)dx=\\int_{-\\infty}^{\\infty}xdF_x(x)$$\n",
    "The expected value for a function $g(\\cdot)$ of a random variable $X$ \n",
    "$$E[g(X)]=\\int_{-\\infty}^{\\infty}g(x)f_X(x)dx$$\n",
    "\n",
    "If two random variables $X$ and $Y$ are independent,\n",
    "$$E[XY]=E[X]E[Y]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important transforms of random variables\n",
    "\n",
    "There are a few handy transforms of random variables that we'll use.\n",
    "\n",
    "## Moment-generating Function\n",
    "\n",
    "The moment-generating function (MGF) for random variable $X$ is given by\n",
    "$$M_X(t)=E[e^{tX}]$$\n",
    "\n",
    "As implied by the name \"moment-generating function\", the MGF allows us to easily calculate the moments of a random variable. To see why, consider its Taylor series expansion around 0. \n",
    "\n",
    "Recall that the Taylor series expansion is  given by\n",
    "\n",
    "$$\n",
    "f(x)=f(a) + \\frac{f'(a)}{1!}(x-a) + \\frac{f''(a)}{2!}(x-a)^2 + \\ldots \n",
    "$$\n",
    "\n",
    "so\n",
    "\n",
    "\\begin{align*}\n",
    "M_X(t) &= E\\left[e^{tX}\\right] \\\\\n",
    " &= E\\left[1 + Xt + \\frac{X^2t^2}{2} + \\frac{X^3t^3}{3!} + \\ldots\\right] \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Now consider the derivatives of the MGF.\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{d}{dt}M_X(t) &= E\\left[X+X^2t+\\frac{X^3t^2}{2}+\\ldots\\right] \\\\\n",
    "\\frac{d^2}{dt^2}M_X(t) &= E\\left[X^2+X^3t+\\ldots\\right] \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Note that by evaluating the MGF at $t=0$ we're left with the $n$th moment.\n",
    "\n",
    "\\begin{align*}\n",
    "\\left.\\frac{d}{dt}M_X(t)\\right\\rvert_{t=0} &= E[X] \\\\\n",
    "\\left.\\frac{d^2}{dt^2}M_X(t)\\right\\rvert_{t=0} &= E[X^2] \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Nifty!\n",
    "\n",
    "## Charateristic Function\n",
    "\n",
    "The characteristic function (CF) for random variable $X$ is given by\n",
    "$$\\phi_X(t)=E[e^{itX}]$$\n",
    "\n",
    "Note that the CF is very closely related to the MGF\n",
    "\n",
    "$$\\phi_X(t)=M_{iX}(t)=M_X(it)$$\n",
    "\n",
    "While the MGF is sometimes easier to work with, it may not always exist, however the CF always exists.\n",
    "\n",
    "You may also note that the CF\n",
    "\n",
    "$$\\phi_X(t)=E[e^{itX}]=\\int_{-\\infty}^\\infty e^{itx}f_X(x)dx$$\n",
    "\n",
    "is exactly the Fourier transform of the random variable's probability density function (PDF). This is important because it tells us that if we know the CF of a RV, then we know its density function simply by passing the CF through the inverse Fourier transform. That is, there is a one-to-one mapping between a RV's PDF and its CF, so \"knowing\" one means we \"know\" the other.\n",
    "\n",
    "## Probability-generating Function\n",
    "\n",
    "The probability-generating function (PGF) for a (discrete) random variable $X$ is given by\n",
    "\n",
    "$$G_X(z)=E[z^X]$$\n",
    "\n",
    "It has useful properties but we won't use it for much other than its existence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sum of Random Variables\n",
    "\n",
    "Now let's say we have random variables $X_i\\sim iid$ and distributed according to $X$\n",
    "\n",
    "Consider their sum $S=\\sum_{i=1}^{N}X_i$\n",
    "\n",
    "First we see that the characteristic function $\\phi_S(s)$ of a sum of $N$ iid random variables is \n",
    "\n",
    "\\begin{align*}\n",
    "\\phi_S(t) &= E[e^{itS}] \\\\\n",
    " &= E[e^{it\\sum_{i=1}^{N}X_i}] \\\\\n",
    " &= E[e^{itX_1}e^{itX_2}\\ldots e^{itX_N}] \\\\\n",
    " &= E[e^{itX_1}]E[e^{itX_2}]\\ldots E[e^{itX_N}] & X_i\\textrm{ are independent}\\\\\n",
    "\\phi_S(t) &= \\phi_X(t)^N & \\textrm{by definition of } \\phi_{X_i}(t)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Sum of Random Variables\n",
    "\n",
    "Now what if $N$ is itself a random variable?\n",
    "\n",
    "\\begin{align*}\n",
    "\\phi_S(t) &= E[e^{itS}] \\\\\n",
    " &= E[e^{it\\sum_{i=1}^{N}X_i}] \\\\\n",
    " &= E[E[e^{it\\sum_{i=1}^{N}X_i}|N]] & \\textrm{by law of total expectation}\\\\\n",
    " &= E[E[e^{it\\sum_{i=1}^{N}X_i}]] & X_i \\textrm{are independent of }N\\\\\n",
    " &= E[E[e^{itX_1}]E[e^{itX_2}]\\ldots E[e^{itX_N}]] & X_i \\textrm{are independent}\\\\\n",
    " &= E[\\phi_X(t)^N] & \\textrm{by definition of } \\phi_{X_i}(t) \\\\\n",
    "\\phi_S(t) &= G_N(\\phi_X(t)) & \\textrm{by definition of } G_N(z)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moments\n",
    "\n",
    "The MGF for the random sum of random variables will have the same form as the CF.\n",
    "\n",
    "$$M_S(t)=G_N(M_X(t))$$\n",
    "\n",
    "Now we can calculate the first moment using the MGF.\n",
    "\n",
    "\\begin{align*}\n",
    "E[S] &= \\left.\\frac{d}{dt}M_S(t)\\right\\rvert_{t=0} \\\\\n",
    " &= \\left.\\frac{d}{dt}G_N(M_X(t))\\right\\rvert_{t=0} \\\\\n",
    " &= \\left[\\left(\\frac{d}{dz}G_N\\right)(M_X(t))\\frac{d}{dt}M_X(t)\\right]_{t=0} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "We now move on to finding each term in our expression for our specific case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $M_X(t)$\n",
    "\n",
    "Let\n",
    "\n",
    "$$X_i(T)=\\frac{1}{\\tau}e^{-(T-t_i)/\\tau}$$\n",
    "\n",
    "\n",
    "where $t_i$ is uniformly distributed between 0 and $T$.\n",
    "\n",
    "\\begin{align*}\n",
    "M_X(t) &= E\\left[e^{tX}\\right] \\\\\n",
    " &= E\\left[e^{t\\frac{1}{\\tau}e^{-(T-t_i)/\\tau}}\\right]\n",
    "\\end{align*}\n",
    "\n",
    "Computing $E\\left[e^{t\\frac{1}{\\tau}e^{-(T-t_i)/\\tau}}\\right]$ directly is difficult (because of the nested exponentials) and, as we'll see, unecessary, so we'll leave it in its current form. Moving on to take the derivative,\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{d}{dt}M_X(t) &= \\frac{d}{dt}E\\left[e^{tX}\\right] \\\\\n",
    " &= E\\left[\\frac{d}{dt}e^{t\\frac{1}{\\tau}e^{-(T-t_i)/\\tau}}\\right] \\\\\n",
    " &= E\\left[\\frac{1}{\\tau}e^{-(T-t_i)/\\tau}e^{t\\frac{1}{\\tau}e^{-(T-t_i)/\\tau}}\\right] \\\\\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $G_N(z)$\n",
    "\n",
    "$N$ comes from a Poisson process with rate $\\lambda$ over a time period $T$, so that $N$ follows a Poisson distribution with parameter $\\lambda T$.\n",
    "\n",
    "$$P\\{N=n\\} = \\frac{(\\lambda T)^ne^{-\\lambda T}}{n!}$$\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{align*}\n",
    "G_N(z) &= E[z^N] \\\\\n",
    "&= \\sum_{n=0}^{\\infty}z^n\\frac{(\\lambda T)^ne^{-\\lambda T}}{n!} \\\\\n",
    "&= e^{-\\lambda T}\\sum_{n=0}^{\\infty}\\frac{(z\\lambda T)^n}{n!} \\\\\n",
    "&= e^{-\\lambda T}e^{z\\lambda T} \\\\\n",
    "G_N(z) &= e^{\\lambda T(z-1)} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "so\n",
    "\n",
    "$$\\frac{d}{dz}G_N(z) = \\lambda Te^{\\lambda T(z-1)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $E[S]$\n",
    "\n",
    "Putting together $M_X(t)$ and $G_N(z)$ \n",
    "\n",
    "\\begin{align*}\n",
    "E[S] &= \\left[\\left(\\frac{d}{dz}G_N\\right)(M_X(t))\\frac{d}{dt}M_X(t)\\right]_{t=0} \\\\\n",
    "&= \\left[\\lambda Te^{\\lambda T\\left(E\\left[e^{t\\frac{1}{\\tau}e^{-(T-t_i)/\\tau}}\\right]-1\\right)}\n",
    "   E\\left[\\frac{1}{\\tau}e^{-(T-t_i)/\\tau}e^{t\\frac{1}{\\tau}e^{-(T-t_i)/\\tau}}\\right]\\right]_{t=0} \\\\\n",
    "&= \\lambda Te^{\\lambda T\\left(E\\left[e^{0}\\right]-1\\right)}\n",
    "   E\\left[\\frac{1}{\\tau}e^{-(T-t_i)/\\tau}e^{0}\\right] & \\textrm{Note how setting }t=0 \\textrm{ cleans things up} \\\\\n",
    "&= \\lambda TE\\left[\\frac{1}{\\tau}e^{-(T-t_i)/\\tau}\\right] \\\\\n",
    "\\end{align*}\n",
    "\n",
    "now\n",
    "\n",
    "\\begin{align*}\n",
    "E\\left[\\frac{1}{\\tau}e^{-(T-t_i)/\\tau}\\right] &= \\frac{e^{-T/\\tau}}{\\tau}E\\left[e^{t_i/\\tau}\\right] \\\\\n",
    "&= \\frac{e^{-T/\\tau}}{\\tau}\\int_0^Te^{t_i/\\tau}\\frac{1}{T}dt_i \\\\\n",
    "&= \\frac{e^{-T/\\tau}}{T\\tau}\\int_0^Te^{t_i/\\tau}dt_i \\\\\n",
    "&= \\frac{e^{-T/\\tau}}{T\\tau}\\left[\\tau e^{t_i/\\tau}\\right]_0^T \\\\\n",
    "&= \\frac{e^{-T/\\tau}}{T}\\left(e^{T/\\tau}-1\\right) \\\\\n",
    "E\\left[\\frac{1}{\\tau}e^{-(T-t_i)/\\tau}\\right] &= \\frac{1}{T}\\left(1-e^{-T/\\tau}\\right) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "so\n",
    "\n",
    "\\begin{align*}\n",
    "E[S] &= \\lambda TE\\left[\\frac{1}{\\tau}e^{-(T-t_i)/\\tau}\\right] \\\\\n",
    "&= \\lambda T\\frac{1}{T}\\left(1-e^{-T/\\tau}\\right) \\\\\n",
    "&= \\lambda \\left(1-e^{-T/\\tau}\\right) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "The mean value of the synapse converges exponentially to the input spike rate with time constant $\\tau$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
