{
 "metadata": {
  "name": "",
  "signature": "sha256:4e6ee7b11bd5552e59424b52bc3aaf655a790601ba696f42e60139bbcc051641"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Notebook description"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "This notebook takes a critical look at the least squares and regularized least squares algorithm in the context of NEF. Specifically:\n",
      " - when is the least-squares algorithm to be used?\n",
      " - when is the regularized least-squares algorithm to be used? When will it break?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Notation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "$\\DeclareMathOperator*{\\argmin}{arg\\,min}$\n",
      "For inputs $\\mathbf{x}$, activity matrix $\\mathbf{A}$, and outputs $\\mathbf{y}$, we desire\n",
      "\n",
      "$$\n",
      "\\mathbf{y}=\\mathbf{A}\\mathbf{x}\n",
      "$$\n",
      "\n",
      "Expanded, \n",
      "\n",
      "$$\n",
      "\\begin{bmatrix}\n",
      "y_0 \\\\\n",
      "y_1 \\\\\n",
      "\\vdots \\\\\n",
      "y_m \\\\\n",
      "\\end{bmatrix}\n",
      "=\n",
      "\\begin{bmatrix}\n",
      "A_{0,0} & A_{0,1} & \\cdots & A_{0,n} \\\\\n",
      "A_{1,0} & A_{1,1} & \\cdots & A_{1,n} \\\\\n",
      "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
      "A_{m,0} & A_{m,1} & \\cdots & A_{m,n} \\\\\n",
      "\\end{bmatrix}\n",
      "\\begin{bmatrix}\n",
      "x_0 \\\\\n",
      "x_1 \\\\\n",
      "\\vdots \\\\\n",
      "x_n \\\\\n",
      "\\end{bmatrix}\n",
      "$$\n",
      "\n",
      "Or put another way,\n",
      "\n",
      "$$\n",
      "\\begin{bmatrix}\n",
      "| \\\\\n",
      "\\mathbf{y} \\\\\n",
      "| \\\\\n",
      "\\end{bmatrix}\n",
      "=\n",
      "\\begin{bmatrix}\n",
      "| & | &  & | \\\\\n",
      "\\mathbf{a}_0 & \\mathbf{a}_1 & \\cdots & \\mathbf{a}_n \\\\\n",
      "| & | &  & | \n",
      "\\end{bmatrix}\n",
      "\\begin{bmatrix}\n",
      "| \\\\\n",
      "\\mathbf{x} \\\\\n",
      "| \\\\\n",
      "\\end{bmatrix}\n",
      "$$\n",
      "\n",
      "where\n",
      "\n",
      "$$\n",
      "\\mathbf{a}_i=\n",
      "\\begin{bmatrix}\n",
      "A_{0,i} \\\\\n",
      "A_{1,i} \\\\\n",
      "\\vdots \\\\\n",
      "A_{m,i} \\\\\n",
      "\\end{bmatrix}\n",
      "$$ \n",
      "\n",
      "is the tuning curve of the $i$th neuron, so\n",
      "\n",
      "$$\n",
      "\\mathbf{y}=\\sum_{i=0}^{n}\\mathbf{a}_ix_i\n",
      "$$\n",
      "\n",
      "$\\|\\cdot\\|_p$ indicates taking the $p$-norm.\n",
      "\n",
      "$\\argmin_{x}(\\cdot)$ indicates the $x$ that minimizes the $(\\cdot)$ expression."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "Least squares"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The goal of least squares is to minimize the error between $\\mathbf{A}\\mathbf{x}$ and $\\mathbf{y}$. We would like the $\\mathbf{a}_i$ to linearly approximate $\\mathbf{y}$ using the weights $\\mathbf{x}$. \n",
      "\n",
      "Error (aka _residuals_) is given by $\\mathbf{e}=\\mathbf{A}\\mathbf{x}-\\mathbf{y}$. We seek $\\mathbf{x}$ to minimize\n",
      "\n",
      "$$\n",
      "\\|\\mathbf{e}\\|_2 = \\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2\n",
      "$$\n",
      "\n",
      "That is, we desire\n",
      "\\begin{align}\n",
      "\\argmin _\\mathbf{x}\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "Derivation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our approach is to take the derivative and solve for the $\\mathbf{x}$ at which the derivative is zero. However, instead of minimizing the norm directly, we will minimize the norm-squared. Since norms are convex and always nonnegative, the $\\mathbf{x}$ that minimizes the norm-squared also minimizes the norm."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Expanding the norm-squared,\n",
      "\n",
      "\\begin{align}\n",
      "\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2^2 &= (\\mathbf{A}\\mathbf{x}-\\mathbf{y})^T(\\mathbf{A}\\mathbf{x}-\\mathbf{y}) \\\\\n",
      " &= (\\mathbf{x}^T\\mathbf{A}^T-\\mathbf{y}^T)(\\mathbf{A}\\mathbf{x}-\\mathbf{y}) \\\\\n",
      " &= \\mathbf{x}^T\\mathbf{A}^T\\mathbf{A}\\mathbf{x}-\\mathbf{x}^T\\mathbf{A}^T\\mathbf{y}-\\mathbf{y}^T\\mathbf{A}\\mathbf{x} \n",
      "    + \\mathbf{y}^T\\mathbf{y} \\\\\n",
      "\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2^2 &=\n",
      " \\mathbf{x}^T\\mathbf{A}^T\\mathbf{A}\\mathbf{x}-2\\mathbf{x}^T\\mathbf{A}^T\\mathbf{y}+\\mathbf{y}^T\\mathbf{y} \\\\\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Taking the derivative,\n",
      "\n",
      "\\begin{align}\n",
      "\\frac{d}{d\\mathbf{x}}\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2^2 &= \\frac{d}{d\\mathbf{x}}\\left(\n",
      " \\mathbf{x}^T\\mathbf{A}^T\\mathbf{A}\\mathbf{x}-2\\mathbf{x}^T\\mathbf{A}^T\\mathbf{y}+\\mathbf{y}^T\\mathbf{y}\\right) \\\\\n",
      " &= 2\\mathbf{A}^T\\mathbf{A}\\mathbf{x}-2\\mathbf{A}^T\\mathbf{y} \\\\\n",
      "\\end{align}\n",
      "\n",
      "and setting to $0$,\n",
      "\n",
      "\\begin{align}\n",
      "0 &= 2\\mathbf{A}^T\\mathbf{A}\\mathbf{x}-2\\mathbf{A}^T\\mathbf{y} \\\\\n",
      "\\mathbf{A}^T\\mathbf{A}\\mathbf{x} &= \\mathbf{A}^T\\mathbf{y} \\\\\n",
      "\\mathbf{x} &= (\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T\\mathbf{y} \\\\\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The error will then be\n",
      "\n",
      "\\begin{align}\n",
      "\\mathbf{e} &= \\mathbf{A}(\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T\\mathbf{y}-\\mathbf{y}\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "L2 Regularized least squares"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}