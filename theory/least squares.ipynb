{
 "metadata": {
  "name": "",
  "signature": "sha256:7919dd0e46b3026f26c6357d693446190183a514248df8d5fd4f65c8fbfa0ba1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Notebook description"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "This notebook takes a critical look at the least squares and regularized least squares algorithm in the context of NEF. Specifically:\n",
      " - when is the least-squares algorithm to be used?\n",
      " - when is the regularized least-squares algorithm to be used? When will it break?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Notation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "$\\DeclareMathOperator*{\\argmin}{arg\\,min}$\n",
      "For inputs $\\mathbf{x}$, activity matrix $\\mathbf{A}$, and outputs $\\mathbf{y}$, we desire\n",
      "\n",
      "$$\n",
      "\\mathbf{y}=\\mathbf{A}\\mathbf{x}\n",
      "$$\n",
      "\n",
      "Expanded, \n",
      "\n",
      "$$\n",
      "\\begin{bmatrix}\n",
      "y_0 \\\\\n",
      "y_1 \\\\\n",
      "\\vdots \\\\\n",
      "y_m \\\\\n",
      "\\end{bmatrix}\n",
      "=\n",
      "\\begin{bmatrix}\n",
      "A_{0,0} & A_{0,1} & \\cdots & A_{0,n} \\\\\n",
      "A_{1,0} & A_{1,1} & \\cdots & A_{1,n} \\\\\n",
      "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
      "A_{m,0} & A_{m,1} & \\cdots & A_{m,n} \\\\\n",
      "\\end{bmatrix}\n",
      "\\begin{bmatrix}\n",
      "x_0 \\\\\n",
      "x_1 \\\\\n",
      "\\vdots \\\\\n",
      "x_n \\\\\n",
      "\\end{bmatrix}\n",
      "$$\n",
      "\n",
      "Or put another way,\n",
      "\n",
      "$$\n",
      "\\begin{bmatrix}\n",
      "| \\\\\n",
      "\\mathbf{y} \\\\\n",
      "| \\\\\n",
      "\\end{bmatrix}\n",
      "=\n",
      "\\begin{bmatrix}\n",
      "| & | &  & | \\\\\n",
      "\\mathbf{a}_0 & \\mathbf{a}_1 & \\cdots & \\mathbf{a}_n \\\\\n",
      "| & | &  & | \n",
      "\\end{bmatrix}\n",
      "\\begin{bmatrix}\n",
      "| \\\\\n",
      "\\mathbf{x} \\\\\n",
      "| \\\\\n",
      "\\end{bmatrix}\n",
      "$$\n",
      "\n",
      "where\n",
      "\n",
      "$$\n",
      "\\mathbf{a}_i=\n",
      "\\begin{bmatrix}\n",
      "A_{0,i} \\\\\n",
      "A_{1,i} \\\\\n",
      "\\vdots \\\\\n",
      "A_{m,i} \\\\\n",
      "\\end{bmatrix}\n",
      "$$ \n",
      "\n",
      "is the tuning curve of the $i$th neuron, so\n",
      "\n",
      "$$\n",
      "\\mathbf{y}=\\sum_{i=0}^{n}\\mathbf{a}_ix_i\n",
      "$$\n",
      "\n",
      "$\\|\\cdot\\|_p$ indicates taking the $p$-norm.\n",
      "\n",
      "$\\argmin_{x}(\\cdot)$ indicates the $x$ that minimizes the $(\\cdot)$ expression."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "Least squares"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The goal of least squares is to minimize the error between $\\mathbf{A}\\mathbf{x}$ and $\\mathbf{y}$. We would like the $\\mathbf{a}_i$ to linearly approximate $\\mathbf{y}$ using the weights $\\mathbf{x}$. \n",
      "\n",
      "Error (aka _residuals_) is given by $\\mathbf{e}=\\mathbf{A}\\mathbf{x}-\\mathbf{y}$. We seek $\\mathbf{x}$ to minimize\n",
      "\n",
      "$$\n",
      "\\|\\mathbf{e}\\|_2 = \\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2\n",
      "$$\n",
      "\n",
      "That is, we desire\n",
      "\\begin{align}\n",
      "\\argmin _\\mathbf{x}\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "Derivation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our approach is to take the derivative and solve for the $\\mathbf{x}$ at which the derivative is zero. However, instead of minimizing the norm directly, we will minimize the norm-squared. Since norms are convex and always nonnegative, the $\\mathbf{x}$ that minimizes the norm-squared also minimizes the norm."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Expanding the norm-squared,\n",
      "\n",
      "\\begin{align}\n",
      "\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2^2 &= (\\mathbf{A}\\mathbf{x}-\\mathbf{y})^T(\\mathbf{A}\\mathbf{x}-\\mathbf{y}) \\\\\n",
      " &= (\\mathbf{x}^T\\mathbf{A}^T-\\mathbf{y}^T)(\\mathbf{A}\\mathbf{x}-\\mathbf{y}) \\\\\n",
      " &= \\mathbf{x}^T\\mathbf{A}^T\\mathbf{A}\\mathbf{x}-\\mathbf{x}^T\\mathbf{A}^T\\mathbf{y}-\\mathbf{y}^T\\mathbf{A}\\mathbf{x} \n",
      "    + \\mathbf{y}^T\\mathbf{y} \\\\\n",
      "\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2^2 &=\n",
      " \\mathbf{x}^T\\mathbf{A}^T\\mathbf{A}\\mathbf{x}-2\\mathbf{x}^T\\mathbf{A}^T\\mathbf{y}+\\mathbf{y}^T\\mathbf{y} \\\\\n",
      "\\end{align}\n",
      "\n",
      "Taking the derivative,\n",
      "\n",
      "\\begin{align}\n",
      "\\frac{d}{d\\mathbf{x}}\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2^2 &= \\frac{d}{d\\mathbf{x}}\\left(\n",
      " \\mathbf{x}^T\\mathbf{A}^T\\mathbf{A}\\mathbf{x}-2\\mathbf{x}^T\\mathbf{A}^T\\mathbf{y}+\\mathbf{y}^T\\mathbf{y}\\right) \\\\\n",
      " &= 2\\mathbf{A}^T\\mathbf{A}\\mathbf{x}-2\\mathbf{A}^T\\mathbf{y} \\\\\n",
      "\\end{align}\n",
      "\n",
      "and setting to $0$,\n",
      "\n",
      "\\begin{align}\n",
      "0 &= 2\\mathbf{A}^T\\mathbf{A}\\mathbf{x}-2\\mathbf{A}^T\\mathbf{y} \\\\\n",
      "\\mathbf{A}^T\\mathbf{A}\\mathbf{x} &= \\mathbf{A}^T\\mathbf{y} \\\\\n",
      "\\mathbf{x} &= (\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T\\mathbf{y} \\\\\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The error will be\n",
      "\n",
      "\\begin{align}\n",
      "\\mathbf{e} &= \\mathbf{A}(\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T\\mathbf{y}-\\mathbf{y}\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's break down the solution,\n",
      "\n",
      "\\begin{align}\n",
      "\\mathbf{A}^T\\mathbf{A} &=\n",
      "    \\begin{bmatrix}\n",
      "    - & \\mathbf{a}_0^T & -\\\\\n",
      "    - & \\mathbf{a}_1^T & -\\\\\n",
      "    - & \\vdots & -\\\\\n",
      "    - & \\mathbf{a}_n^T & -\\\\\n",
      "    \\end{bmatrix}\n",
      "    \\begin{bmatrix}\n",
      "    | & | & | & | \\\\\n",
      "    \\mathbf{a}_0 & \\mathbf{a}_1 & \\cdots & \\mathbf{a}_n \\\\\n",
      "    | & | & | & | \\\\\n",
      "    \\end{bmatrix} \\\\\n",
      "&=\n",
      "    \\begin{bmatrix}\n",
      "    \\mathbf{a}_0^T\\mathbf{a}_0 & \\mathbf{a}_0^T\\mathbf{a}_1 & \\cdots & \\mathbf{a}_0^T\\mathbf{a}_n\\\\\n",
      "    \\mathbf{a}_1^T\\mathbf{a}_0 & \\mathbf{a}_1^T\\mathbf{a}_1 & \\cdots & \\mathbf{a}_1^T\\mathbf{a}_n\\\\\n",
      "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
      "    \\mathbf{a}_n^T\\mathbf{a}_0 & \\mathbf{a}_n^T\\mathbf{a}_1 & \\cdots & \\mathbf{a}_n^T\\mathbf{a}_n\\\\\n",
      "    \\end{bmatrix} \\\\\n",
      "&=\n",
      "    \\begin{bmatrix}\n",
      "    \\sum_{i=0}^mA_{0,i}^2 & \\sum_{i=0}^mA_{0,i}A_{1,i} & \\cdots & \\sum_{i=0}^mA_{0,i}A_{n,i}\\\\\n",
      "    \\sum_{i=0}^mA_{1,i}A_{0,i} & \\sum_{i=0}^mA_{1,i}^2 & \\cdots & \\sum_{i=0}^mA_{1,i}A_{n, i}\\\\\n",
      "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
      "    \\sum_{i=0}^mA_{n,i}A_{0,i} & \\sum_{i=0}^mA_{n,i}A_{1,i} & \\cdots & \\sum_{i=0}^mA_{n,i}^2\\\\\n",
      "    \\end{bmatrix}\n",
      "\\end{align}\n",
      "\n",
      "These are the pairwise projections of the tuning curves onto each other."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align}\n",
      "\\mathbf{A}^T\\mathbf{y} &= \n",
      "    \\begin{bmatrix}\n",
      "    - & \\mathbf{a}_0^T & -\\\\\n",
      "    - & \\mathbf{a}_1^T & -\\\\\n",
      "    - & \\vdots & -\\\\\n",
      "    - & \\mathbf{a}_n^T & -\\\\\n",
      "    \\end{bmatrix}\n",
      "    \\mathbf{y} \\\\\n",
      " &= \n",
      "    \\begin{bmatrix}\n",
      "    \\mathbf{a}_0^T\\mathbf{y} \\\\\n",
      "    \\mathbf{a}_1^T\\mathbf{y} \\\\\n",
      "    \\vdots \\\\\n",
      "    \\mathbf{a}_n^T\\mathbf{y} \\\\\n",
      "    \\end{bmatrix} \\\\\n",
      " &=\n",
      "    \\begin{bmatrix}\n",
      "    \\sum_{i=0}^mA_{0,i}y_i \\\\\n",
      "    \\sum_{i=0}^mA_{1,i}y_i \\\\\n",
      "    \\vdots \\\\\n",
      "    \\sum_{i=0}^mA_{n,i}y_i \\\\\n",
      "    \\end{bmatrix} \\\\\n",
      "\\end{align}\n",
      "This is the projection of the output onto the tuning curves."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Least squares with noise"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We could consider noise in $\\mathbf{A}$, $\\mathbf{x}$, or $\\mathbf{y}$. However, given that we specify $\\mathbf{x}$ and $\\mathbf{y}$, noise will only be present in $\\mathbf{A}$, so here we analyze noise in $\\mathbf{A}$. Noise in the output $\\mathbf{y}$ is considered in the Supplementary material of this notebook. Noise in $\\mathbf{A}$ arises because of spikes and how we obtain $\\mathbf{A}$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let $\\mathbf{A}=A+\\eta$, where $A$ describes the noiseless tuning curves and $\\eta$ is the noise.\n",
      "\n",
      "Expanding the norm-squared,\n",
      "\n",
      "\\begin{align}\n",
      "\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2^2 &= (\\mathbf{A}\\mathbf{x}-\\mathbf{y})^T(\\mathbf{A}\\mathbf{x}-\\mathbf{y}) \\\\\n",
      " &= (\\mathbf{x}^T\\mathbf{A}^T-\\mathbf{y}^T)(\\mathbf{A}\\mathbf{x}-\\mathbf{y}) \\\\\n",
      " &= \\mathbf{x}^T\\mathbf{A}^T\\mathbf{A}\\mathbf{x}-\\mathbf{x}^T\\mathbf{A}^T\\mathbf{y}-\\mathbf{y}^T\\mathbf{A}\\mathbf{x} \n",
      "    + \\mathbf{y}^T\\mathbf{y} \\\\\n",
      "\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2^2 &=\n",
      " \\mathbf{x}^T\\mathbf{A}^T\\mathbf{A}\\mathbf{x}-2\\mathbf{x}^T\\mathbf{A}^T\\mathbf{y}+\\mathbf{y}^T\\mathbf{y} \\\\\n",
      "\\end{align}\n",
      "\n",
      "Taking the derivative and expanding the noise terms,\n",
      "\n",
      "\\begin{align}\n",
      "\\frac{d}{d\\mathbf{x}}\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2^2 &= \\frac{d}{d\\mathbf{x}}\\left(\n",
      " \\mathbf{x}^T\\mathbf{A}^T\\mathbf{A}\\mathbf{x}-2\\mathbf{x}^T\\mathbf{A}^T\\mathbf{y}+\\mathbf{y}^T\\mathbf{y}\\right) \\\\\n",
      " &= 2\\mathbf{A}^T\\mathbf{A}\\mathbf{x}-2\\mathbf{A}^T\\mathbf{y} \\\\\n",
      " &= 2(A^T+\\eta^T)(A+\\eta)\\mathbf{x}-2(A^T+\\eta^T)\\mathbf{y} \\\\\n",
      "\\end{align}\n",
      "\n",
      "and setting the expected value to $0$,\n",
      "\n",
      "\\begin{align}\n",
      "0 &= E\\left[2(A^T+\\eta^T)(A+\\eta)\\mathbf{x}-2(A^T+\\eta^T)\\mathbf{y}\\right] \\\\\n",
      " &= E\\left[(A^T+\\eta^T)(A+\\eta)\\mathbf{x}-(A^T+\\eta^T)\\mathbf{y}\\right] \\\\\n",
      " &= E\\left[A^TA\\mathbf{x}+A^T\\eta\\mathbf{x}+\\eta^TA\\mathbf{x}+\\eta^T\\eta\\mathbf{x}-A^T\\mathbf{y}-\\eta^T\\mathbf{y}\\right] \\\\\n",
      " &= A^TA\\mathbf{x}+A^TE[\\eta]\\mathbf{x}+E[\\eta]^TA\\mathbf{x}+E[\\eta^T\\eta]\\mathbf{x}-A^T\\mathbf{y}-E[\\eta]^T\\mathbf{y} \\\\\n",
      " &= A^TA\\mathbf{x}+E[\\eta^T\\eta]\\mathbf{x}-A^T\\mathbf{y} \\\\\n",
      " &= A^TA\\mathbf{x}+\\sigma^2I\\mathbf{x}-A^T\\mathbf{y} \\\\\n",
      "(A^TA+\\sigma^2I)\\mathbf{x} &= A^T\\mathbf{y} \\\\\n",
      "\\mathbf{x} &= (A^TA+\\sigma^2I)A^T\\mathbf{y} \\\\\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Supplementary material"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Least squares with output noise"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here, we will see that when unbiased noise is present in the output $\\mathbf{y}$ leads to the original least squares solution.\n",
      "\n",
      "Let $\\mathbf{y}=y+\\eta$, and assume that $E[\\eta]=0$.\n",
      "\n",
      "Expanding the norm-squared,\n",
      "\n",
      "\\begin{align}\n",
      "\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2^2 &= (\\mathbf{A}\\mathbf{x}-\\mathbf{y})^T(\\mathbf{A}\\mathbf{x}-\\mathbf{y}) \\\\\n",
      " &= (\\mathbf{x}^T\\mathbf{A}^T-\\mathbf{y}^T)(\\mathbf{A}\\mathbf{x}-\\mathbf{y}) \\\\\n",
      " &= \\mathbf{x}^T\\mathbf{A}^T\\mathbf{A}\\mathbf{x}-\\mathbf{x}^T\\mathbf{A}^T\\mathbf{y}-\\mathbf{y}^T\\mathbf{A}\\mathbf{x} \n",
      "    + \\mathbf{y}^T\\mathbf{y} \\\\\n",
      "\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2^2 &=\n",
      " \\mathbf{x}^T\\mathbf{A}^T\\mathbf{A}\\mathbf{x}-2\\mathbf{x}^T\\mathbf{A}^T\\mathbf{y}+\\mathbf{y}^T\\mathbf{y} \\\\\n",
      "\\end{align}\n",
      "\n",
      "Taking the derivative,\n",
      "\n",
      "\\begin{align}\n",
      "\\frac{d}{d\\mathbf{x}}\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2^2 &= \\frac{d}{d\\mathbf{x}}\\left(\n",
      " \\mathbf{x}^T\\mathbf{A}^T\\mathbf{A}\\mathbf{x}-2\\mathbf{x}^T\\mathbf{A}^T\\mathbf{y}+\\mathbf{y}^T\\mathbf{y}\\right) \\\\\n",
      " &= 2\\mathbf{A}^T\\mathbf{A}\\mathbf{x}-2\\mathbf{A}^T\\mathbf{y} \\\\\n",
      " &= 2\\mathbf{A}^T\\mathbf{A}\\mathbf{x}-2\\mathbf{A}^T(y+\\eta) \\\\\n",
      "\\end{align}\n",
      "\n",
      "and setting the expected value to $0$,\n",
      "\n",
      "\\begin{align}\n",
      "0 &= E\\left[2\\mathbf{A}^T\\mathbf{A}\\mathbf{x}-2\\mathbf{A}^T(y+\\eta)\\right] \\\\\n",
      " &= 2\\mathbf{A}^T\\mathbf{A}\\mathbf{x}-2\\mathbf{A}^T(y+E[\\eta]) \\\\\n",
      " &= 2\\mathbf{A}^T\\mathbf{A}\\mathbf{x}-2\\mathbf{A}^Ty \\\\\n",
      "\\mathbf{A}^T\\mathbf{A}\\mathbf{x} &= \\mathbf{A}^Ty \\\\\n",
      "\\mathbf{x} &= (\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^Ty \\\\\n",
      "\\end{align}\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}