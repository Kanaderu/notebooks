{
 "metadata": {
  "name": "",
  "signature": "sha256:5a7fd47b8553581a24cce9545d9756e3e55a423927379ab9ca05de6ef7ac6d1c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Notebook description"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "This notebook examines the least squares and regularized least squares algorithm, particularly in the context of NEF. \n",
      "\n",
      "\n",
      "In least squares, our goal is to approximate a desired vector, $\\mathbf{y}$, as the linear combination of vectors $\\mathbf{a}_i$, using weights $\\mathbf{x}$. That is, we would like\n",
      "\n",
      "$$\n",
      "\\mathbf{y}\\approx\\mathbf{A}\\mathbf{x}\n",
      "$$\n",
      "\n",
      "To find the best approximation of $\\mathbf{y}$, the least squares technique formulates the problem as\n",
      "\n",
      "$$\n",
      "\\min\\|\\mathbf{y}-\\mathbf{A}\\mathbf{x}\\|_2\n",
      "$$\n",
      "\n",
      "or rather finding $\\hat{\\mathbf{x}}$ where\n",
      "\n",
      "$\\DeclareMathOperator*{\\argmin}{arg\\,min}$\n",
      "$$\n",
      "\\hat{\\mathbf{x}}=\\argmin_{\\mathbf{x}}\\|\\mathbf{y}-\\mathbf{A}\\mathbf{x}\\|_2\n",
      "$$\n",
      "\n",
      "Specifically, we would like to know:\n",
      " - When should least-squares be used?\n",
      " - When should regularized least-squares algorithm be used? When will it break?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Notation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$\\DeclareMathOperator*{\\argmin}{arg\\,min}$\n",
      "For $\\mathbf{x}\\in\\mathbb{R}^{n}$, $\\mathbf{A}\\in\\mathbb{R}^{m\\times n}$, and outputs $\\mathbf{y}\\in\\mathbb{R}^{m}$, we desire\n",
      "\n",
      "$$\n",
      "\\mathbf{y}=\\mathbf{A}\\mathbf{x}\n",
      "$$\n",
      "\n",
      "Expanded, \n",
      "\n",
      "$$\n",
      "\\begin{bmatrix}\n",
      "y_0 \\\\\n",
      "y_1 \\\\\n",
      "\\vdots \\\\\n",
      "y_m \\\\\n",
      "\\end{bmatrix}\n",
      "=\n",
      "\\begin{bmatrix}\n",
      "A_{0,0} & A_{0,1} & \\cdots & A_{0,n} \\\\\n",
      "A_{1,0} & A_{1,1} & \\cdots & A_{1,n} \\\\\n",
      "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
      "A_{m,0} & A_{m,1} & \\cdots & A_{m,n} \\\\\n",
      "\\end{bmatrix}\n",
      "\\begin{bmatrix}\n",
      "x_0 \\\\\n",
      "x_1 \\\\\n",
      "\\vdots \\\\\n",
      "x_n \\\\\n",
      "\\end{bmatrix}\n",
      "$$\n",
      "\n",
      "Or put another way,\n",
      "\n",
      "$$\n",
      "\\begin{bmatrix}\n",
      "| \\\\\n",
      "\\mathbf{y} \\\\\n",
      "| \\\\\n",
      "\\end{bmatrix}\n",
      "=\n",
      "\\begin{bmatrix}\n",
      "| & | &  & | \\\\\n",
      "\\mathbf{a}_0 & \\mathbf{a}_1 & \\cdots & \\mathbf{a}_n \\\\\n",
      "| & | &  & | \n",
      "\\end{bmatrix}\n",
      "\\begin{bmatrix}\n",
      "| \\\\\n",
      "\\mathbf{x} \\\\\n",
      "| \\\\\n",
      "\\end{bmatrix}\n",
      "$$\n",
      "\n",
      "where\n",
      "\n",
      "$$\n",
      "\\mathbf{a}_i=\n",
      "\\begin{bmatrix}\n",
      "A_{0,i} \\\\\n",
      "A_{1,i} \\\\\n",
      "\\vdots \\\\\n",
      "A_{m,i} \\\\\n",
      "\\end{bmatrix}\n",
      "$$ \n",
      "\n",
      "so\n",
      "\n",
      "$$\n",
      "\\mathbf{y}=\\sum_{i=1}^{n}\\mathbf{a}_ix_i\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "In statistics, $\\mathbf{y}$ is called the regressand, $\\mathbf{a}_i$ are called regressors, and $\\mathbf{x}$ are the regression coefficients.\n",
      "\n",
      "In NEF,  $\\mathbf{y}$ consists of sample points of the desired function, $\\mathbf{a}_i$ are the neural tuning curves, and $\\mathbf{x}$ are the decoding weights. However, you may see different notation used in NEF texts. In NEF texts, $\\mathbf{x}$ represents the input to the neurons, so $\\mathbf{a}_i$ becomes $\\mathbf{a}_i(\\mathbf{x})$ and $\\phi$ is used instead of $\\mathbf{x}$ to represent the decoding weights. $\\mathbf{y}$ becomes $f(\\mathbf{x})$ as it consists of samples from a function of $\\mathbf{x}$. Putting these changes together using NEF notation, we would write $f(\\mathbf{x})=\\sum_{i=1}^{n}\\mathbf{a}_i(\\mathbf{x})\\phi_i$ instead of $\\mathbf{y}=\\sum_{i=1}^{n}\\mathbf{a}_ix_i$.\n",
      "\n",
      "As a final point of notation:\n",
      " - $\\|\\cdot\\|_p$ indicates taking the $p$-norm.\n",
      " - $\\argmin_{x}(\\cdot)$ indicates the $x$ that minimizes the $(\\cdot)$ expression."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "Least squares"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The goal of least squares is to minimize the error between $\\mathbf{A}\\mathbf{x}$ and $\\mathbf{y}$. We would like the $\\mathbf{a}_i$ to linearly approximate $\\mathbf{y}$ using $\\mathbf{x}$. \n",
      "\n",
      "Error (aka _residuals_) is given by $\\mathbf{e}=\\mathbf{A}\\mathbf{x}-\\mathbf{y}$. We seek $\\mathbf{x}$ to minimize\n",
      "\n",
      "$$\n",
      "\\|\\mathbf{e}\\|_2 = \\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2\n",
      "$$\n",
      "\n",
      "That is, we desire\n",
      "\\begin{align}\n",
      "\\argmin _\\mathbf{x}\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "Derivation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our approach is to take the derivative and solve for the $\\mathbf{x}$ at which the derivative is zero. However, instead of minimizing the norm directly, we will minimize the norm-squared. Since norms are convex and always nonnegative, the $\\mathbf{x}$ that minimizes the norm-squared also minimizes the norm."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Expanding the norm-squared,\n",
      "\n",
      "\\begin{align}\n",
      "\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2^2 &= (\\mathbf{A}\\mathbf{x}-\\mathbf{y})^T(\\mathbf{A}\\mathbf{x}-\\mathbf{y}) \\\\\n",
      " &= (\\mathbf{x}^T\\mathbf{A}^T-\\mathbf{y}^T)(\\mathbf{A}\\mathbf{x}-\\mathbf{y}) \\\\\n",
      " &= \\mathbf{x}^T\\mathbf{A}^T\\mathbf{A}\\mathbf{x}-\\mathbf{x}^T\\mathbf{A}^T\\mathbf{y}-\\mathbf{y}^T\\mathbf{A}\\mathbf{x} \n",
      "    + \\mathbf{y}^T\\mathbf{y} \\\\\n",
      "\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2^2 &=\n",
      " \\mathbf{x}^T\\mathbf{A}^T\\mathbf{A}\\mathbf{x}-2\\mathbf{x}^T\\mathbf{A}^T\\mathbf{y}+\\mathbf{y}^T\\mathbf{y} \\\\\n",
      "\\end{align}\n",
      "\n",
      "Taking the derivative,\n",
      "\n",
      "\\begin{align}\n",
      "\\frac{d}{d\\mathbf{x}}\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2^2 &= \\frac{d}{d\\mathbf{x}}\\left(\n",
      " \\mathbf{x}^T\\mathbf{A}^T\\mathbf{A}\\mathbf{x}-2\\mathbf{x}^T\\mathbf{A}^T\\mathbf{y}+\\mathbf{y}^T\\mathbf{y}\\right) \\\\\n",
      " &= 2\\mathbf{A}^T\\mathbf{A}\\mathbf{x}-2\\mathbf{A}^T\\mathbf{y} \\\\\n",
      "\\end{align}\n",
      "\n",
      "and setting to $0$,\n",
      "\n",
      "\\begin{align}\n",
      "0 &= 2\\mathbf{A}^T\\mathbf{A}\\mathbf{x}-2\\mathbf{A}^T\\mathbf{y} \\\\\n",
      "\\mathbf{A}^T\\mathbf{A}\\mathbf{x} &= \\mathbf{A}^T\\mathbf{y} \\\\\n",
      "\\mathbf{x} &= (\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T\\mathbf{y} \\\\\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's break down the solution,\n",
      "\n",
      "\\begin{align}\n",
      "\\mathbf{A}^T\\mathbf{A} &=\n",
      "    \\begin{bmatrix}\n",
      "    - & \\mathbf{a}_0^T & -\\\\\n",
      "    - & \\mathbf{a}_1^T & -\\\\\n",
      "    - & \\vdots & -\\\\\n",
      "    - & \\mathbf{a}_n^T & -\\\\\n",
      "    \\end{bmatrix}\n",
      "    \\begin{bmatrix}\n",
      "    | & | & | & | \\\\\n",
      "    \\mathbf{a}_0 & \\mathbf{a}_1 & \\cdots & \\mathbf{a}_n \\\\\n",
      "    | & | & | & | \\\\\n",
      "    \\end{bmatrix} \\\\\n",
      "&=\n",
      "    \\begin{bmatrix}\n",
      "    \\mathbf{a}_0^T\\mathbf{a}_0 & \\mathbf{a}_0^T\\mathbf{a}_1 & \\cdots & \\mathbf{a}_0^T\\mathbf{a}_n\\\\\n",
      "    \\mathbf{a}_1^T\\mathbf{a}_0 & \\mathbf{a}_1^T\\mathbf{a}_1 & \\cdots & \\mathbf{a}_1^T\\mathbf{a}_n\\\\\n",
      "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
      "    \\mathbf{a}_n^T\\mathbf{a}_0 & \\mathbf{a}_n^T\\mathbf{a}_1 & \\cdots & \\mathbf{a}_n^T\\mathbf{a}_n\\\\\n",
      "    \\end{bmatrix} \\\\\n",
      "&=\n",
      "    \\begin{bmatrix}\n",
      "    \\sum_{i=0}^mA_{0,i}^2 & \\sum_{i=0}^mA_{0,i}A_{1,i} & \\cdots & \\sum_{i=0}^mA_{0,i}A_{n,i}\\\\\n",
      "    \\sum_{i=0}^mA_{1,i}A_{0,i} & \\sum_{i=0}^mA_{1,i}^2 & \\cdots & \\sum_{i=0}^mA_{1,i}A_{n, i}\\\\\n",
      "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
      "    \\sum_{i=0}^mA_{n,i}A_{0,i} & \\sum_{i=0}^mA_{n,i}A_{1,i} & \\cdots & \\sum_{i=0}^mA_{n,i}^2\\\\\n",
      "    \\end{bmatrix}\n",
      "\\end{align}\n",
      "\n",
      "In the context of NEF, these are the pairwise projections of the tuning curves onto each other."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align}\n",
      "\\mathbf{A}^T\\mathbf{y} &= \n",
      "    \\begin{bmatrix}\n",
      "    - & \\mathbf{a}_0^T & -\\\\\n",
      "    - & \\mathbf{a}_1^T & -\\\\\n",
      "    - & \\vdots & -\\\\\n",
      "    - & \\mathbf{a}_n^T & -\\\\\n",
      "    \\end{bmatrix}\n",
      "    \\mathbf{y} \\\\\n",
      " &= \n",
      "    \\begin{bmatrix}\n",
      "    \\mathbf{a}_0^T\\mathbf{y} \\\\\n",
      "    \\mathbf{a}_1^T\\mathbf{y} \\\\\n",
      "    \\vdots \\\\\n",
      "    \\mathbf{a}_n^T\\mathbf{y} \\\\\n",
      "    \\end{bmatrix} \\\\\n",
      " &=\n",
      "    \\begin{bmatrix}\n",
      "    \\sum_{i=0}^mA_{0,i}y_i \\\\\n",
      "    \\sum_{i=0}^mA_{1,i}y_i \\\\\n",
      "    \\vdots \\\\\n",
      "    \\sum_{i=0}^mA_{n,i}y_i \\\\\n",
      "    \\end{bmatrix} \\\\\n",
      "\\end{align}\n",
      "In the context of NEF, this is the projection of the output onto the tuning curves.\n",
      "\n",
      "[Here](http://nbviewer.ipython.org/github/fragapanagos/notebooks/blob/master/theory/least_squares_error.ipynb) is a discussion of the error in least squares."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Least squares with noise"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We could consider noise in $\\mathbf{A}$, $\\mathbf{x}$, or $\\mathbf{y}$. However, in NEF, given that we specify $\\mathbf{x}$ and $\\mathbf{y}$, noise will only be present in $\\mathbf{A}$, so here we analyze noise in $\\mathbf{A}$. Noise in the output $\\mathbf{y}$ is considered in the Supplementary material of this notebook. Noise in $\\mathbf{A}$ arises because of spikes and how we obtain $\\mathbf{A}$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let $\\mathbf{A}=A+\\eta$, where \n",
      "\n",
      "$$\n",
      "A =\n",
      "\\begin{bmatrix}\n",
      "| & | &  & | \\\\\n",
      "a_0 & a_1 & \\cdots & a_n \\\\\n",
      "| & | &  & | \n",
      "\\end{bmatrix}\n",
      "$$ \n",
      "\n",
      "and \n",
      "\n",
      "\\begin{align}\n",
      "\\eta &=\n",
      "    \\begin{bmatrix}\n",
      "    | & | &  & | \\\\\n",
      "    \\eta_0 & \\eta_1 & \\cdots & \\eta_n \\\\\n",
      "    | & | &  & | \n",
      "    \\end{bmatrix} \\\\\n",
      " &= \n",
      "    \\begin{bmatrix}\n",
      "    \\eta_{0,0} & \\eta_{0,1} & \\cdots & \\eta_{0,n} \\\\\n",
      "    \\eta_{1,0} & \\eta_{1,1} & \\cdots & \\eta_{1,n} \\\\\n",
      "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
      "    \\eta_{m,0} & \\eta_{m,1} & \\cdots & \\eta_{m,n} \\\\\n",
      "    \\end{bmatrix} \\\\\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$a_i$ describes the mean tuning curve of the $i$th neuron. Beware that the _mean_ tuning curve is not necessarily the same as the _noiseless_ tuning curve. It really depends on the noise at the input to the neuron. $\\eta_{j,k}$ describes the $k$th neuron's noise centered on its mean tuning curve value at the $j$th sample of the tuning curve.  \n",
      "\n",
      "Modeling the mean tuning curve as equivalent to the noiseless tuning curve implies that noise only arises from our estimation of the tuning curve. That is, when a neuron spikes at the rate specified by its noiseless tuning curve, we estimate the tuning curve rate by filtering the spikes. The estimated rate will be noisy, but unbiased around the specified noiseless tuning curve firing rate. The estimated rates will be unbiased regardless of the specific noise distribution, which depends on whether the neurons produce spikes with Poisson statistics, uniformly, or anything in between.\n",
      "\n",
      "However, noise in our tuning curve estimates also arises because of noise at the inputs to the neurons. For example, an LIF neuron with constant input current  spikes uniformly. However, if the LIF neuron's input current comes from a synapse, which is noisy, it will not spike uniformly anymore nor will it spike at the same rate as we would expect given the mean input current. That is, noise in the neuron's input current effectively \"smooths\" the tuning curves, so if we treat the mean tuning curves as equivalent to the noiseless tuning curves, we necessarily introduce a bias in the noise to account for the tuning curve smoothing and so $E[\\eta]\\neq0$. We will see that the math is simplified if we let $a_i$ be the mean tuning curve instead of the noiseless tuning curve so that $E[\\eta]=0$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Expanding the norm-squared,\n",
      "\n",
      "\\begin{align}\n",
      "\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2^2 &= (\\mathbf{A}\\mathbf{x}-\\mathbf{y})^T(\\mathbf{A}\\mathbf{x}-\\mathbf{y}) \\\\\n",
      " &= (\\mathbf{x}^T\\mathbf{A}^T-\\mathbf{y}^T)(\\mathbf{A}\\mathbf{x}-\\mathbf{y}) \\\\\n",
      " &= \\mathbf{x}^T\\mathbf{A}^T\\mathbf{A}\\mathbf{x}-\\mathbf{x}^T\\mathbf{A}^T\\mathbf{y}-\\mathbf{y}^T\\mathbf{A}\\mathbf{x} \n",
      "    + \\mathbf{y}^T\\mathbf{y} \\\\\n",
      "\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2^2 &=\n",
      " \\mathbf{x}^T\\mathbf{A}^T\\mathbf{A}\\mathbf{x}-2\\mathbf{x}^T\\mathbf{A}^T\\mathbf{y}+\\mathbf{y}^T\\mathbf{y} \\\\\n",
      "\\end{align}\n",
      "\n",
      "Taking the derivative and expanding the noise terms,\n",
      "\n",
      "\\begin{align}\n",
      "\\frac{d}{d\\mathbf{x}}\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2^2 &= \\frac{d}{d\\mathbf{x}}\\left(\n",
      " \\mathbf{x}^T\\mathbf{A}^T\\mathbf{A}\\mathbf{x}-2\\mathbf{x}^T\\mathbf{A}^T\\mathbf{y}+\\mathbf{y}^T\\mathbf{y}\\right) \\\\\n",
      " &= 2\\mathbf{A}^T\\mathbf{A}\\mathbf{x}-2\\mathbf{A}^T\\mathbf{y} \\\\\n",
      " &= 2(A^T+\\eta^T)(A+\\eta)\\mathbf{x}-2(A^T+\\eta^T)\\mathbf{y} \\\\\n",
      "\\end{align}\n",
      "\n",
      "and setting the expected value to $0$,\n",
      "\n",
      "\\begin{align}\n",
      "0 &= E\\left[2(A^T+\\eta^T)(A+\\eta)\\mathbf{x}-2(A^T+\\eta^T)\\mathbf{y}\\right] \\\\\n",
      " &= E\\left[(A^T+\\eta^T)(A+\\eta)\\mathbf{x}-(A^T+\\eta^T)\\mathbf{y}\\right] \\\\\n",
      " &= E\\left[A^TA\\mathbf{x}+A^T\\eta\\mathbf{x}+\\eta^TA\\mathbf{x}+\\eta^T\\eta\\mathbf{x}-A^T\\mathbf{y}-\\eta^T\\mathbf{y}\\right] \\\\\n",
      " &= A^TA\\mathbf{x}+A^TE[\\eta]\\mathbf{x}+E[\\eta]^TA\\mathbf{x}+E[\\eta^T\\eta]\\mathbf{x}-A^T\\mathbf{y}-E[\\eta]^T\\mathbf{y} \\\\\n",
      "\\end{align}\n",
      "\n",
      "At this point, we will consider the different cases for $E[\\eta]$ and $E[\\eta^T\\eta]$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If $E[n]=0$\n",
      "\n",
      "\\begin{align}\n",
      "0 &= A^TA\\mathbf{x}+A^TE[\\eta]\\mathbf{x}+E[\\eta]^TA\\mathbf{x}+E[\\eta^T\\eta]\\mathbf{x}-A^T\\mathbf{y}-E[\\eta]^T\\mathbf{y} \\\\ \n",
      " &= A^TA\\mathbf{x}+E[\\eta^T\\eta]\\mathbf{x}-A^T\\mathbf{y} & \\quad \\text{since }E[\\eta]=0\\\\\n",
      " &= A^TA\\mathbf{x}+E[\\eta^T\\eta]\\mathbf{x}-A^T\\mathbf{y} \\\\\n",
      "(A^TA+E[\\eta^T\\eta])\\mathbf{x} &= A^T\\mathbf{y} \\\\\n",
      "\\mathbf{x} &= (A^TA+E[\\eta^T\\eta])^{-1}A^T\\mathbf{y} \\\\\n",
      "\\end{align}\n",
      "\n",
      "If, in addition, $E[\\eta^T\\eta]=\\sigma^2I$,\n",
      "\n",
      "\\begin{align}\n",
      "\\mathbf{x} &= \\left(A^TA+\\sigma^2I\\right)^{-1}A^T\\mathbf{y} \\\\\n",
      "\\end{align}\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Keep in mind what these assumptions imply about the noise.\n",
      " - $E[\\eta]=0$ implies that the noise is unbiased, which is only true if _$a_i$ is the mean tuning curves rather than the noiseless tuning curve_.\n",
      " - $E[\\eta^T\\eta]=\\sigma^2I$ implies that the noise is _uncorrelated between neurons across the input space_, which may or may not be true.\n",
      " \n",
      "\\begin{align}\n",
      "E[\\eta^T\\eta] &=\n",
      "    E\\begin{bmatrix}\n",
      "    \\eta_0^T\\eta_0 & \\eta_0^T\\eta_1 & \\cdots & \\eta_0^T\\eta_n\\\\\n",
      "    \\eta_1^T\\eta_0 & \\eta_1^T\\eta_1 & \\cdots & \\eta_1^T\\eta_n\\\\\n",
      "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
      "    \\eta_n^T\\eta_0 & \\eta_n^T\\eta_1 & \\cdots & \\eta_n^T\\eta_n\\\\\n",
      "    \\end{bmatrix} \\\\\n",
      "&=\n",
      "    \\begin{bmatrix}\n",
      "    \\sum_{i=0}^mE[\\eta_{0,i}^2] & \\sum_{i=0}^mE[\\eta_{0,i}\\eta_{1,i}] & \\cdots & \\sum_{i=0}^mE[\\eta_{0,i}\\eta_{n,i}]\\\\\n",
      "    \\sum_{i=0}^mE[\\eta_{1,i}\\eta_{0,i}] & \\sum_{i=0}^mE[\\eta_{1,i}^2] & \\cdots & \\sum_{i=0}^mE[\\eta_{1,i}\\eta_{n, i}]\\\\\n",
      "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
      "    \\sum_{i=0}^mE[\\eta_{n,i}\\eta_{0,i}] & \\sum_{i=0}^mE[\\eta_{n,i}\\eta_{1,i}] & \\cdots & \\sum_{i=0}^mE[\\eta_{n,i}^2]\\\\\n",
      "    \\end{bmatrix}\n",
      "\\end{align}\n",
      "\n",
      "If $\\eta_{i,j}$ is uncorrelated with $\\eta_{k,l}$ for $i\\neq k$ and $j\\neq l$, then the off diagonal terms are 0, so\n",
      "\n",
      "\\begin{align}\n",
      "E[\\eta^T\\eta] &=\n",
      "    \\begin{bmatrix}\n",
      "    \\sum_{i=0}^mE[\\eta_{0,i}^2] & 0 & \\cdots & 0 \\\\\n",
      "    0 & \\sum_{i=0}^mE[\\eta_{1,i}^2] & \\cdots & 0 \\\\\n",
      "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
      "    0 & 0 & \\cdots & \\sum_{i=0}^mE[\\eta_{n,i}^2]\\\\\n",
      "    \\end{bmatrix}\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "However, in the most general case where $E[\\eta]\\neq0$ and $E[\\eta^T\\eta]\\neq\\sigma^2I$,\n",
      "\n",
      "\\begin{align}\n",
      "0 &= A^TA\\mathbf{x}+A^TE[\\eta]\\mathbf{x}+E[\\eta]^TA\\mathbf{x}+E[\\eta^T\\eta]\\mathbf{x}-A^T\\mathbf{y}-E[\\eta]^T\\mathbf{y} \\\\\n",
      "\\left(A^TA+A^TE[\\eta]+E[\\eta]^TA+E[\\eta^T\\eta]\\right)\\mathbf{x} &= \\left(A^T+E[\\eta]^T\\right)\\mathbf{y} \\\\\n",
      "\\mathbf{x} &= \\left(A^TA+A^TE[\\eta]+E[\\eta]^TA+E[\\eta^T\\eta]\\right)^{-1}\\left(A^T+E[\\eta]^T\\right)\\mathbf{y} \\\\\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Supplementary material"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Least squares with output noise"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here, we will see that when unbiased noise is present in the output $\\mathbf{y}$ leads to the original least squares solution.\n",
      "\n",
      "Let $\\mathbf{y}=y+\\eta$, and assume that $E[\\eta]=0$.\n",
      "\n",
      "Starting with the expanding the norm-squared,\n",
      "\n",
      "\\begin{align}\n",
      "\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2^2 &=\n",
      " \\mathbf{x}^T\\mathbf{A}^T\\mathbf{A}\\mathbf{x}-2\\mathbf{x}^T\\mathbf{A}^T\\mathbf{y}+\\mathbf{y}^T\\mathbf{y} \\\\\n",
      "\\end{align}\n",
      "\n",
      "Taking the derivative,\n",
      "\n",
      "\\begin{align}\n",
      "\\frac{d}{d\\mathbf{x}}\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2^2 &= \\frac{d}{d\\mathbf{x}}\\left(\n",
      " \\mathbf{x}^T\\mathbf{A}^T\\mathbf{A}\\mathbf{x}-2\\mathbf{x}^T\\mathbf{A}^T\\mathbf{y}+\\mathbf{y}^T\\mathbf{y}\\right) \\\\\n",
      " &= 2\\mathbf{A}^T\\mathbf{A}\\mathbf{x}-2\\mathbf{A}^T\\mathbf{y} \\\\\n",
      " &= 2\\mathbf{A}^T\\mathbf{A}\\mathbf{x}-2\\mathbf{A}^T(y+\\eta) \\\\\n",
      "\\end{align}\n",
      "\n",
      "and setting the expected value to $0$,\n",
      "\n",
      "\\begin{align}\n",
      "0 &= E\\left[2\\mathbf{A}^T\\mathbf{A}\\mathbf{x}-2\\mathbf{A}^T(y+\\eta)\\right] \\\\\n",
      " &= 2\\mathbf{A}^T\\mathbf{A}\\mathbf{x}-2\\mathbf{A}^T(y+E[\\eta]) \\\\\n",
      " &= 2\\mathbf{A}^T\\mathbf{A}\\mathbf{x}-2\\mathbf{A}^Ty \\\\\n",
      "\\mathbf{A}^T\\mathbf{A}\\mathbf{x} &= \\mathbf{A}^Ty \\\\\n",
      "\\mathbf{x} &= (\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^Ty \\\\\n",
      "\\end{align}\n",
      "\n",
      "but what if we don't have access to $y$?...How much error do we introduce?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Regularized least squares SVD analysis "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we will show that the least squares solution via singular value decomposition (SVD) is the same as regularized least squares for constant noise power. By taking the singular value decomposition $A=U\\Sigma V^T$,\n",
      "\n",
      "\\begin{align}\n",
      "\\mathbf{x} &= \\left(A^TA+\\sigma^2I\\right)^{-1}A^T\\mathbf{y} \\\\\n",
      " &= \\left(V\\Sigma U^TU\\Sigma V^T+\\sigma^2I\\right)^{-1}V\\Sigma U^T\\mathbf{y} \\\\\n",
      " &= \\left(V\\Sigma^2V^T+\\sigma^2I\\right)^{-1}V\\Sigma U^T\\mathbf{y} \\\\\n",
      " &= \\left(V\\Sigma^2V^T+\\sigma^2VV^T\\right)^{-1}V\\Sigma U^T\\mathbf{y} \\\\\n",
      " &= \\left(V\\left(\\Sigma^2+\\sigma^2I\\right)V^T\\right)^{-1}V\\Sigma U^T\\mathbf{y} \\\\\n",
      " &= V\\left(\\Sigma^2+\\sigma^2I\\right)^{-1}V^TV\\Sigma U^T\\mathbf{y} \\\\\n",
      " &= V\\left(\\Sigma^2+\\sigma^2I\\right)^{-1}\\Sigma U^T\\mathbf{y} \\\\\n",
      " &= VDU^T\\mathbf{y} & \\text{where } D \\text{ is diagonal and } D_{i,i}=\\frac{\\alpha_i}{\\alpha_i^2+\\sigma^2}\\\\\n",
      "\\end{align}\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Regularized least squares vs $A^TA$ SVD conditioning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If instead of using the regularized least squares approach, we use SVD of $A^TA$ to condition the inverse.\n",
      "\n",
      "If $A=U\\Sigma V^T$, \n",
      "\n",
      "$$\n",
      "A^TA=V\\Sigma U^TU\\Sigma V^T=V\\Sigma^2V^T,\n",
      "$$\n",
      "\n",
      "and \n",
      "\n",
      "\\begin{align}\n",
      "\\mathbf{x} &= (\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T\\mathbf{y} \\\\\n",
      " &= (V\\Sigma^2V^T)^{-1}\\mathbf{A}^T\\mathbf{y} \\\\\n",
      " &= V\\Sigma^{-2}V^T\\mathbf{A}^T\\mathbf{y} \\\\\n",
      "\\end{align}\n",
      "\n",
      "When computing $\\Sigma^{-2}$, we clip the entries of $\\Sigma$ that are within $\\epsilon$ of $0$ and only invert the resulting nonzero entries.\n",
      "\n",
      "Open questions and issues:\n",
      " - Show the relation between this method and regularized least squares.\n",
      "  - What is the realtion between the clipping parameters $\\epsilon$ and the regularization parameter $\\sigma$?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Least squares (pseudoinverse) via SVD"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we will derive the least squares solution using SVD.\n",
      "\n",
      "\\begin{align}\n",
      "\\mathbf{A}\\mathbf{x} &= \\mathbf{y} \\\\\n",
      "U\\Sigma V^T\\mathbf{x} &= \\mathbf{y} \\\\\n",
      "\\Sigma V^T\\mathbf{x} &= U^T\\mathbf{y} \\\\\n",
      "V^T\\mathbf{x} &= \\Sigma^{-1}U^T\\mathbf{y} \\\\\n",
      "\\mathbf{x} &= V\\Sigma^{-1}U^T\\mathbf{y} \\\\\n",
      "\\end{align}\n",
      "\n",
      "Open questions and issues:\n",
      " - Show the relation between this method and regularized least squares."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Tikhanov regularization derivation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The regularized least squares solution also arises from formulating the problem as finding\n",
      "\n",
      "$$\n",
      "\\argmin_x \\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2^2+\\|\\mathbf{\\Gamma}\\mathbf{x}\\|_2^2\n",
      "$$\n",
      "\n",
      "where $\\mathbf{\\Gamma}$ is called a Tikhanov matrix.\n",
      "\n",
      "Expanding the objective function,\n",
      "\n",
      "\\begin{align}\n",
      "\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2^2+\\|\\mathbf{\\Gamma}\\mathbf{x}\\|_2^2 &=\n",
      "  \\mathbf{x}^T\\mathbf{A}^T\\mathbf{A}\\mathbf{x}-2\\mathbf{x}^T\\mathbf{A}^T\\mathbf{y}+\\mathbf{y}^T\\mathbf{y}+\n",
      "  \\mathbf{x}^T\\mathbf{\\Gamma}^T\\mathbf{\\Gamma}\\mathbf{x} \\\\\n",
      "\\end{align}\n",
      "\n",
      "Taking the derivative,\n",
      "\n",
      "\\begin{align}\n",
      "\\frac{d}{d\\mathbf{x}}\\left(\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2^2+\\|\\mathbf{\\Gamma}\\mathbf{x}\\|_2^2\\right) &=\n",
      "   2\\mathbf{A}^T\\mathbf{A}\\mathbf{x}-2\\mathbf{A}^T\\mathbf{y}+2\\mathbf{\\Gamma}^T\\mathbf{\\Gamma}\\mathbf{x} \\\\\n",
      "\\end{align}\n",
      "\n",
      "Setting the derivative equal to 0,\n",
      "\n",
      "\\begin{align}\n",
      "0 &= 2\\mathbf{A}^T\\mathbf{A}\\mathbf{x}-2\\mathbf{A}^T\\mathbf{y}+2\\mathbf{\\Gamma}^T\\mathbf{\\Gamma}\\mathbf{x} \\\\\n",
      "\\left(\\mathbf{A}^T\\mathbf{A}+\\mathbf{\\Gamma}^T\\mathbf{\\Gamma}\\right)\\mathbf{x} &= \\mathbf{A}^T\\mathbf{y} \\\\\n",
      "\\mathbf{x} &= \\left(\\mathbf{A}^T\\mathbf{A}+\\mathbf{\\Gamma}^T\\mathbf{\\Gamma}\\right)^{-1}\\mathbf{A}^T\\mathbf{y} \\\\\n",
      "\\end{align}\n",
      "\n",
      "This solution is the same as the regularized least squares formulation with $E[\\eta]=0$, $\\mathbf{A}=A$, and $\\mathbf{\\Gamma}^T\\mathbf{\\Gamma}=E[\\eta^T\\eta]$.\n",
      "\n",
      "$$\n",
      "\\mathbf{x} = \\left(A^TA+E[\\eta^T\\eta]\\right)^{-1}A^T\\mathbf{y}\n",
      "$$"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}