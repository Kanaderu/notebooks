{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook examines the error that results from the [least squares](http://nbviewer.ipython.org/github/fragapanagos/notebooks/blob/master/theory/least_squares.ipynb).\n",
    "\n",
    "In least squares, our goal is to approximate a desired vector, $\\mathbf{y}$, as the linear combination of vectors $\\mathbf{a}_i$, using weights $\\mathbf{x}$. That is, we would like\n",
    "\n",
    "$$\n",
    "\\mathbf{y}\\approx\\mathbf{A}\\mathbf{x}\n",
    "$$\n",
    "\n",
    "We will see that under some conditions $\\mathbf{A}\\mathbf{x}=\\mathbf{y}$ exactly, but under other conditions we can only approximate $\\mathbf{y}$. To find the best approximation of $\\mathbf{y}$, the least squares technique formulates the problem as\n",
    "\n",
    "$$\n",
    "\\min\\|\\mathbf{y}-\\mathbf{A}\\mathbf{x}\\|_2\n",
    "$$\n",
    "\n",
    "or rather finding $\\hat{\\mathbf{x}}$ where\n",
    "\n",
    "$\\DeclareMathOperator*{\\argmin}{arg\\,min}$\n",
    "$$\n",
    "\\hat{\\mathbf{x}}=\\argmin_{\\mathbf{x}}\\|\\mathbf{y}-\\mathbf{A}\\mathbf{x}\\|_2\n",
    "$$\n",
    "\n",
    "\n",
    "Specifically, we would like to know: \n",
    " - How accurate is our approximation of $\\mathbf{y}$? That is, what is $\\|\\mathbf{y}-\\mathbf{A}\\hat{\\mathbf{x}}\\|_2$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case where there is no noise in $\\mathbf{A}$ or $\\mathbf{y}$,\n",
    "\n",
    "\\begin{align}\n",
    "\\|\\mathbf{y}-\\mathbf{A}\\hat{\\mathbf{x}}\\|_2 &= \\left\\|\\mathbf{U}_{\\sigma=0}^T\\mathbf{y}\\right\\|_2 \\\\\n",
    " &= \\left\\|\\sum_{i=r+1}^m\\mathbf{u}_i\\mathbf{u}_i^T\\mathbf{y}\\right\\|_2 \\\\\n",
    "\\end{align}\n",
    "\n",
    "In the noisy case ... TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The desired function is described with $m$ sample points and could be a single-dimensional or multi-dimensional function. In the single dimensional case, $\\mathbf{y}\\in\\mathbb{R}^{m}$, $\\mathbf{A}\\in\\mathbb{R}^{m\\times n}$,  $\\mathbf{x}\\in\\mathbb{R}^{n}$, and\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "y_0 \\\\\n",
    "y_1 \\\\\n",
    "\\vdots \\\\\n",
    "y_m \\\\\n",
    "\\end{bmatrix}\n",
    "\\approx\n",
    "\\begin{bmatrix}\n",
    "A_{0,0} & A_{0,1} & \\cdots & A_{0,n} \\\\\n",
    "A_{1,0} & A_{1,1} & \\cdots & A_{1,n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "A_{m,0} & A_{m,1} & \\cdots & A_{m,n} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_0 \\\\\n",
    "x_1 \\\\\n",
    "\\vdots \\\\\n",
    "x_n \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Or put another way,\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "| \\\\\n",
    "\\mathbf{y} \\\\\n",
    "| \\\\\n",
    "\\end{bmatrix}\n",
    "\\approx\n",
    "\\begin{bmatrix}\n",
    "| & | &  & | \\\\\n",
    "\\mathbf{a}_0 & \\mathbf{a}_1 & \\cdots & \\mathbf{a}_n \\\\\n",
    "| & | &  & | \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "| \\\\\n",
    "\\mathbf{x} \\\\\n",
    "| \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In the multidimensional case, $\\mathbf{y}\\in\\mathbb{R}^{m\\times d}$, $\\mathbf{A}\\in\\mathbb{R}^{m\\times n}$, and $\\mathbf{x}\\in\\mathbb{R}^{n\\times d}$. However the multidimensional case is [separable](#Separability-of-decoding-dimensions), so we need only analyze the single dimensional case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our analysis centers on the properties of $\\mathbf{A}$ in relation to the operation $\\mathbf{A}\\mathbf{x}$, which is a linear map between two vector spaces $\\mathbb{R}^n$ and $\\mathbb{R}^m$ where $\\mathbf{x}\\in\\mathbb{R}^n$ and $\\mathbf{y}\\in\\mathbb{R}^m$. This linear map has four fundamental subspaces:\n",
    " - The set of vectors $\\mathbf{y}\\in\\mathbb{R}^m$ such that $\\mathbf{y}=\\mathbf{A}\\mathbf{x}$ for any $\\mathbf{x}\\in\\mathbb{R}^n$ is the _column space_ of $\\mathbf{A}$. The dimensionality of the column space is $r$, the _rank_ of $\\mathbf{A}$.\n",
    " - The set of vectors $\\mathbf{x}\\in\\mathbb{R}^n$ such that $\\mathbf{A}^T\\mathbf{y}=\\mathbf{x}$ for any $\\mathbf{y}\\in\\mathbb{R}^m$ is the _row space_ of $\\mathbf{A}$. The dimensionality of the row space is also $r$.\n",
    " - The set of vectors $\\mathbf{x}\\in\\mathbb{R}^n$ such that $\\mathbf{0}=\\mathbf{A}\\mathbf{x}$ is the _nullspace_. The dimensionality of the nullspace is $(n-r)$, the _nullity_ of $\\mathbf{A}$. The nullspace is the complement of the row space.\n",
    " - The set of vectors $\\mathbf{y}\\in\\mathbb{R}^m$ such that $\\mathbf{A}^T\\mathbf{y}=0$ is the _left nullspace_. The dimensionality of the left nullspace is $(m-r)$, the corank of $\\mathbf{A}$. The left null space is the complement of the column space.\n",
    "\n",
    "More specifically, our analysis will focus on the left null space of $\\mathbf{A}$. The column space and the left nullspace are complementary subsets of $\\mathbb{R}^m$, so they span $\\mathbb{R}^m$ and divide it into two subspaces. The column space is the space of all vectors that we could possibly reach for some $\\mathbf{x}$ with $\\mathbf{A}\\mathbf{x}$. The left nullspace is the space of all vectors that are impossible for us to reach using $\\mathbf{A}\\mathbf{x}$ for any $\\mathbf{x}$. At this point, we can begin to intuit where error from our linear approximation of $\\mathbf{y}$ will come from.  If there $\\mathbf{y}$ has components in the left null space of $\\mathbf{A}$, then it will be impossible for us to recreate $\\mathbf{y}$ exactly for any $\\mathbf{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To express $\\mathbf{y}$ in terms of the column space and the left null space, we will use the SVD. $\\mathbf{A}$ can be decomposed via SVD as\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{A} &= \\mathbf{U}\\Sigma\\mathbf{V}^T \\\\\n",
    " &= \\sum_{i=1}^{r}\\sigma_i\\mathbf{u}_i\\mathbf{v}_i^T \\\\\n",
    "\\end{align}\n",
    "\n",
    "where\n",
    " - $\\mathbf{U}\\in\\mathbb{R}^{m\\times m}$\n",
    " - $\\mathbf{V}\\in\\mathbb{R}^{n\\times n}$\n",
    " - $\\Sigma\\in\\mathbb{R}^{m\\times n}$\n",
    " \n",
    "and\n",
    " - the columns of $\\mathbf{U}$ (the _left singular vectors_) form an orthogonal basis for $\\mathbb{R}^m$,\n",
    " - the columns of $\\mathbf{V}$ (the _right singular vectors_) form an orthogonal basis for $\\mathbb{R}^n$,\n",
    " - $\\Sigma$ is diagonal rectangular with diagonal entries $\\sigma_i$, and the number of nonzero $\\sigma_i$ is $r$. \n",
    "\n",
    "Therefore\n",
    " - the right singular vectors corresponding to the zero entries of $\\Sigma$ form an orthogonal basis for the nullspace of $\\mathbf{A}$,\n",
    " - the right singular vectors corresponding to the nonzero entries of $\\Sigma$ form an orthogonal basis for the row space of $\\mathbf{A}$,\n",
    " - the left singular vectors corresponding to the zero entries of $\\Sigma$ form an orthogonal basis for the left nullspace of $\\mathbf{A}$.\n",
    " - the left singular vectors corresponding to the nonzero entries of $\\Sigma$ form an orthogonal basis for the column space of $\\mathbf{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The least squares-optimal $\\mathbf{x}$, $\\hat{\\mathbf{x}}$, that minimizes $\\|\\mathbf{y}-\\mathbf{A}\\mathbf{x}\\|_2$ is given by the pseudoinverse\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{x}}=\\mathbf{A}^+\\mathbf{y}\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "\\mathbf{A}^+=\\mathbf{V}\\Sigma^+\\mathbf{U}^T\n",
    "$$\n",
    "\n",
    "and $\\Sigma^+$ is computed by setting the nonzero entries of $\\Sigma$ to their reciprocal, leaving the $0$s in place, and transposing the matrix, so $\\Sigma^+\\in\\mathbb{R}^{n\\times m}$. Also,\n",
    " - In the case that the columns of $\\mathbf{A}$ are linearly independent, \n",
    " $\\mathbf{A}^+=(\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T$\n",
    " - In the case that the rows of $\\mathbf{A}$ are linearly independent,\n",
    " $\\mathbf{A}^+=\\mathbf{A}^T(\\mathbf{A}\\mathbf{A}^T)^{-1}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the SVD, we can determine exactly how much error we can expect when approximating $\\mathbf{y}$ using $\\mathbf{A}\\mathbf{x}$. The error in our approximation will be exactly captured in the left null space of $\\mathbf{A}$. That is, we cannot possibly reach anywhere within the left nullspace using $\\mathbf{A}\\mathbf{x}$ for any $\\mathbf{x}\\in\\mathbb{R}^n$. Therefore, we should consider the projection of $\\mathbf{y}$ onto the columns of $\\mathbf{U}$ that form a basis for the left null space of $\\mathbf{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column space and the left null space projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This projection is expressed as \n",
    "\\begin{align}\n",
    "\\mathbf{y} &= \\mathbf{U}\\mathbf{U}^T\\mathbf{y} \\\\\n",
    " &= \\sum_{i=1}^mu_iu_i^Ty \\\\\n",
    " &= \\sum_{i=1}^ru_iu_i^Ty + \\sum_{j=r+1}^mu_ju_j^Ty \\\\\n",
    "\\end{align}\n",
    "\n",
    "where we've separated $u_i$ into vectors in the column space of $\\mathbf{A}$ and vectors in the left null space of $\\mathbf{A}$. Another way of expressing this is\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{U}(\\mathbf{U}_{\\sigma\\neq0}^T\\mathbf{y}+\\mathbf{U}_{\\sigma=0}^T\\mathbf{y})\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "\\mathbf{U}_{\\sigma\\neq0} =\n",
    "\\begin{bmatrix}\n",
    "| & & | & | & & | \\\\\n",
    "u_1 & \\cdots & u_r & 0 & \\cdots & 0 \\\\ \n",
    "| & & | & | & & | \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "\\mathbf{U}_{\\sigma=0} =\n",
    "\\begin{bmatrix}\n",
    "| & & | & | & & | \\\\\n",
    "0 & \\cdots & 0 & u_{r+1} & \\cdots & u_m \\\\ \n",
    "| & & | & | & & | \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "That is,\n",
    " - $\\mathbf{U}_{\\sigma\\neq0}$ contains the left singular vectors corresponding to the nonzero entries of $\\Sigma$, and \n",
    " - $\\mathbf{U}_{\\sigma=0}$ contains the left singular vectors corresponding to the zero entries of $\\Sigma$.\n",
    "\n",
    "\n",
    "Projecting $\\mathbf{y}$ onto $\\mathbf{U}_{\\sigma\\neq0}$ and $\\mathbf{U}_{\\sigma=0}$ expresses $\\mathbf{y}$ as the combination of a column space projection and a left null space projection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the pseudoinverse solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting aside our column space and left null space projection of $\\mathbf{y}$ for the moment, we now analyze the least squares solution obtained using the pseudoinverse:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{x}} = \\mathbf{A}^+\\mathbf{y}\n",
    "$$\n",
    "\n",
    "so \n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{A}\\hat{\\mathbf{x}} &= \\mathbf{A}\\mathbf{A}^+\\mathbf{y} \\\\\n",
    " &= \\mathbf{U}\\Sigma\\mathbf{V}^T\\mathbf{V}\\Sigma^+\\mathbf{U}^T\\mathbf{y} \\\\\n",
    " &= \\mathbf{U}\\Sigma\\Sigma^+\\mathbf{U}^T\\mathbf{y} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we need to be careful. What becomes of $\\Sigma\\Sigma^+$ depends on the rank and dimensions of $\\mathbf{A}$. Recall that $\\Sigma\\in\\mathbb{R}^{m\\times n}$ and $\\Sigma^+\\in\\mathbb{R}^{n\\times m}$ are both diagonal rectangular with $r$ nonzero entries, so $\\Sigma\\Sigma^+\\in\\mathbb{R}^{m\\times m}$ and has $r$-$1$s along the diagonal. $r\\le m$ always so we have two cases to consider: $r=m$ and $r<m$. \n",
    "\n",
    "When $r=m$, $\\Sigma\\Sigma^+=\\mathbf{I}$, so\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{A}\\hat{\\mathbf{x}} &= \\mathbf{U}\\Sigma\\Sigma^+\\mathbf{U}^T\\mathbf{y} \\\\\n",
    " &= \\mathbf{U}\\mathbf{I}\\mathbf{U}^T\\mathbf{y} \\\\\n",
    " &= \\mathbf{U}\\mathbf{U}^T\\mathbf{y} \\\\\n",
    " &= \\mathbf{y} \\\\\n",
    "\\end{align}\n",
    "\n",
    "At this point, we see that when $r=m$, we can exactly reconstruct $\\mathbf{y}$ using $\\mathbf{A}\\hat{\\mathbf{x}}$ for any $\\mathbf{y}$, so our approximation error will be \n",
    "\n",
    "\\begin{align}\n",
    "\\|\\mathbf{y}-\\mathbf{A}\\hat{\\mathbf{x}}\\|_2 &= \\|\\mathbf{y}-\\mathbf{y}\\|_2 \\\\\n",
    "&= 0 \\\\\n",
    "\\end{align}\n",
    "\n",
    "When $r<m$, $\\Sigma\\Sigma^+=\\mathbf{I}_r$ where $\\mathbf{I}_r$ has only $r$ ones along the diagonal, so\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{A}\\hat{\\mathbf{x}} &= \\mathbf{U}\\Sigma\\Sigma^+\\mathbf{U}^T\\mathbf{y} \\\\\n",
    " &= \\mathbf{U}\\mathbf{I}_r\\mathbf{U}^T\\mathbf{y} \\\\\n",
    " &= \\sum_{i=1}^m\\mathbf{i}^r_{i}\\mathbf{u}_i\\mathbf{u}_i^T\\mathbf{y} \\\\\n",
    " &= \\sum_{i=1}^r\\mathbf{i}^r_{i}\\mathbf{u}_i\\mathbf{u}_i^T\\mathbf{y}+\n",
    "     \\sum_{i=r+1}^m\\mathbf{i}^r_{i}\\mathbf{u}_i\\mathbf{u}_i^T\\mathbf{y} \\\\\n",
    " &= \\sum_{i=1}^r\\mathbf{u}_i\\mathbf{u}_i^T\\mathbf{y} \\\\\n",
    "\\end{align}\n",
    "\n",
    "Another way of saying this is\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{A}\\hat{\\mathbf{x}} &= \\mathbf{U}\\Sigma\\Sigma^+\\mathbf{U}^T\\mathbf{y} \\\\\n",
    " &= \\mathbf{U}\\mathbf{I}_r\\mathbf{U}^T\\mathbf{y} \\\\\n",
    " &= \\mathbf{U}\\mathbf{U}_{\\sigma\\neq0}^T\\mathbf{y} \\\\\n",
    "\\end{align}\n",
    "\n",
    "At this point, [recall](#Column-space-and-the-left-null-space-projections) our projections of $\\mathbf{y}$ onto the column space and the left null space of $\\mathbf{A}$. We see that when $r<m$, we can only reconstruct the components of $\\mathbf{y}$ that fall within the column space of $\\mathbf{A}$, so our approximation error will be\n",
    "\n",
    "\\begin{align}\n",
    "\\|\\mathbf{y}-\\mathbf{A}\\hat{\\mathbf{x}}\\|_2 &= \n",
    "    \\left\\|\\mathbf{U}(\\mathbf{U}_{\\sigma\\neq0}^T\\mathbf{y}+\\mathbf{U}_{\\sigma=0}^T\\mathbf{y})\n",
    "    -\\mathbf{U}\\mathbf{U}_{\\sigma\\neq0}^T\\mathbf{y}\\right\\|_2 \\\\\n",
    " &= \\left\\|\\mathbf{U}\\mathbf{U}_{\\sigma=0}^T\\mathbf{y}\\right\\|_2 \\\\\n",
    " &= \\left\\|\\mathbf{U}_{\\sigma=0}^T\\mathbf{y}\\right\\|_2 & \\mathbf{U} \\text{ is a rotation matrix}\\\\\n",
    "\\end{align}\n",
    "\n",
    "which is the norm of $\\mathbf{y}$ projected onto the left null space of $\\mathbf{A}$. Put in summation notation,\n",
    "\n",
    "$$\n",
    "\\|\\mathbf{y}-\\mathbf{A}\\hat{\\mathbf{x}}\\|_2 = \n",
    "    \\left\\|\\sum_{i=r+1}^m\\mathbf{u}_i\\mathbf{u}_i^T\\mathbf{y}\\right\\|_2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEF applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEF uses least squares to linearly approximate a desired function using the weighted combination of neural tuning curves. The function is sampled at $m$ points and we have $n$ neurons. Since neurons are typically heterogenous, no tuning curve is a scaled version of another tuning curve, so $\\mathbf{A}$ is typically full rank. This means that if $m\\le n$, $r=m$ and $\\mathbf{A}\\hat{\\mathbf{x}}=\\mathbf{y}$ exactly. \n",
    "\n",
    "However, remember that our goal is to approximate the function underlying $\\mathbf{y}$ more than just the particular points of $\\mathbf{y}$. If we only use a few sample points to find the weights, then we are assuming that the tuning curves and desired function are linear between sample points. Even if the desired function is linear, tuning curves are generally nonlinear, so this assumption fails and we would perform poorly when approximating points of the function that were not sampled during training. Therefore, we typically set $m\\gg n$, in which case $r=n$. Since $m\\gg r$, the left nullspace will be very large compared to column space, and it will be very possible that our desired function has nonzero projections in the left null space of our activity matrix.\n",
    "\n",
    "TODO\n",
    " - what do left singular vectors of tuning curves look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separability of decoding dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With no other constraints on $\\mathbf{x}$,\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "| & | & & | \\\\\n",
    "\\mathbf{y}_0 & \\mathbf{y}_1 & \\cdots & \\mathbf{y}_d \\\\\n",
    "| & | & & | \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "| & | &  & | \\\\\n",
    "\\mathbf{a}_0 & \\mathbf{a}_1 & \\cdots & \\mathbf{a}_n \\\\\n",
    "| & | &  & | \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "| & | & & | \\\\\n",
    "\\mathbf{x}_0 & \\mathbf{x}_1 & \\cdots & \\mathbf{x}_d \\\\\n",
    "| & | & & | \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "can be separated into\n",
    "\\begin{align}\n",
    "\\begin{bmatrix}\n",
    "    | \\\\\n",
    "    \\mathbf{y}_0 \\\\\n",
    "    | \\\\\n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "    | & | &  & | \\\\\n",
    "    \\mathbf{a}_0 & \\mathbf{a}_1 & \\cdots & \\mathbf{a}_n \\\\\n",
    "    | & | &  & | \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    | \\\\\n",
    "    \\mathbf{x}_0 \\\\\n",
    "    | \\\\\n",
    "\\end{bmatrix} \\\\ \\\\\n",
    "\\begin{bmatrix}\n",
    "    | \\\\\n",
    "    \\mathbf{y}_1 \\\\\n",
    "    | \\\\\n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "    | & | &  & | \\\\\n",
    "    \\mathbf{a}_0 & \\mathbf{a}_1 & \\cdots & \\mathbf{a}_n \\\\\n",
    "    | & | &  & | \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    | \\\\\n",
    "    \\mathbf{x}_1 \\\\\n",
    "    | \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "&\\ \\vdots \\\\\n",
    "\\begin{bmatrix}\n",
    "    | \\\\\n",
    "    \\mathbf{y}_d \\\\\n",
    "    | \\\\\n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "    | & | &  & | \\\\\n",
    "    \\mathbf{a}_0 & \\mathbf{a}_1 & \\cdots & \\mathbf{a}_n \\\\\n",
    "    | & | &  & | \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    | \\\\\n",
    "    \\mathbf{x}_d \\\\\n",
    "    | \\\\\n",
    "\\end{bmatrix} \\\\ \\\\\n",
    "\\end{align}\n",
    "\n",
    "where we can consider each equation separately.\n",
    "\n",
    "Note that there are times when we do impose constraints on $\\mathbf{x}$. For example, when using the generalized form of least squares to find $\\mathbf{x}$, we seek\n",
    "\n",
    "$\\DeclareMathOperator*{\\argmin}{arg\\,min}$\n",
    "$$\n",
    "\\argmin_x \\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2^2+\\|\\mathbf{\\Gamma}\\mathbf{x}\\|_2^2\n",
    "$$\n",
    "\n",
    "Here, $\\mathbf{\\Gamma}$ could impose a relationship between the columns of $\\mathbf{x}$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
