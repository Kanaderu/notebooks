{
 "metadata": {
  "name": "",
  "signature": "sha256:3ae0111379863bdbac60fe885206f853fe75bbb7d8281e8b1326b9384caeafc3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Notebook description"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook examines our ability to approximate arbitrary functions linearly. \n",
      "\n",
      "In linear function approximation, our goal is to approximate a desired function, $\\mathbf{y}$, as the linear combination of approximator functions, $\\mathbf{a}_i$, using weights $\\mathbf{x}$. That is, \n",
      "\n",
      "$$\n",
      "\\mathbf{y}\\approx\\mathbf{A}\\mathbf{x}\n",
      "$$\n",
      "\n",
      "Specifically, we would like to know: Given a regressand and set of regressors, how well could our regressors approximate the regressand?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Summary of results"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "...punchline goes here..."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Notation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The desired function is described with $m$ sample points and could be a single-dimensional or multi-dimensional function. In the single dimensional case,  $\\mathbf{y}\\in\\mathbb{R}^{m}$, $\\mathbf{A}\\in\\mathbb{R}^{m\\times n}$,  $\\mathbf{x}\\in\\mathbb{R}^{n}$, and\n",
      "\n",
      "$$\n",
      "\\begin{bmatrix}\n",
      "y_0 \\\\\n",
      "y_1 \\\\\n",
      "\\vdots \\\\\n",
      "y_m \\\\\n",
      "\\end{bmatrix}\n",
      "\\approx\n",
      "\\begin{bmatrix}\n",
      "A_{0,0} & A_{0,1} & \\cdots & A_{0,n} \\\\\n",
      "A_{1,0} & A_{1,1} & \\cdots & A_{1,n} \\\\\n",
      "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
      "A_{m,0} & A_{m,1} & \\cdots & A_{m,n} \\\\\n",
      "\\end{bmatrix}\n",
      "\\begin{bmatrix}\n",
      "x_0 \\\\\n",
      "x_1 \\\\\n",
      "\\vdots \\\\\n",
      "x_n \\\\\n",
      "\\end{bmatrix}\n",
      "$$\n",
      "\n",
      "Or put another way,\n",
      "\n",
      "$$\n",
      "\\begin{bmatrix}\n",
      "| \\\\\n",
      "\\mathbf{y} \\\\\n",
      "| \\\\\n",
      "\\end{bmatrix}\n",
      "\\approx\n",
      "\\begin{bmatrix}\n",
      "| & | &  & | \\\\\n",
      "\\mathbf{a}_0 & \\mathbf{a}_1 & \\cdots & \\mathbf{a}_n \\\\\n",
      "| & | &  & | \n",
      "\\end{bmatrix}\n",
      "\\begin{bmatrix}\n",
      "| \\\\\n",
      "\\mathbf{x} \\\\\n",
      "| \\\\\n",
      "\\end{bmatrix}\n",
      "$$\n",
      "\n",
      "In the multidimensional case, $\\mathbf{y}\\in\\mathbb{R}^{m\\times d}$, $\\mathbf{A}\\in\\mathbb{R}^{m\\times n}$, and $\\mathbf{x}\\in\\mathbb{R}^{n\\times d}$. However the multidimensional case is [separable](#Separability-of-decoding-dimensions), so we need only analyze the single dimensional case."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As an aside, in statistics, $\\mathbf{y}$ is called the regressand, $\\mathbf{a}_i$ are called regressors, and $\\mathbf{x}$ are the regression coefficients."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Background"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our analysis centers on the properties of $\\mathbf{A}$ in relation to the operation $\\mathbf{A}\\mathbf{x}$, which is a linear map between two vector spaces $\\mathbb{R}^n$ and $\\mathbb{R}^m$ where $\\mathbf{x}\\in\\mathbb{R}^n$ and $\\mathbf{y}\\in\\mathbb{R}^m$. This linear map has four fundamental subspaces:\n",
      " - The set of vectors $\\mathbf{y}\\in\\mathbb{R}^m$ such that $\\mathbf{y}=\\mathbf{A}\\mathbf{x}$ for any $\\mathbf{x}\\in\\mathbb{R}^n$ is the _column space_ of $\\mathbf{A}$. The dimensionality of the column space is $r$, the _rank_ of $\\mathbf{A}$.\n",
      " - The set of vectors $\\mathbf{x}\\in\\mathbb{R}^n$ such that $\\mathbf{A}^T\\mathbf{y}=\\mathbf{x}$ for any $\\mathbf{y}\\in\\mathbb{R}^m$ is the _row space_ of $\\mathbf{A}$. The dimensionality of the row space is also $r$.\n",
      " - The set of vectors $\\mathbf{x}\\in\\mathbb{R}^n$ such that $\\mathbf{0}=\\mathbf{A}\\mathbf{x}$ is the _nullspace_. The dimensionality of the nullspace is $(n-r)$, the _nullity_ of $\\mathbf{A}$. The nullspace is the complement of the row space.\n",
      " - The set of vectors $\\mathbf{y}\\in\\mathbb{R}^m$ such that $\\mathbf{A}^T\\mathbf{y}=0$ is the _left nullspace_. The dimensionality of the left nullspace is $(m-r)$, the corank of $\\mathbf{A}$. The left null space is the complement of the column space.\n",
      "\n",
      "More specifically, our analysis will focus on the left null space of $\\mathbf{A}$. The column space and the left nullspace are complementary subsets of $\\mathbb{R}^m$, so they span $\\mathbb{R}^m$ and divide it into two subspaces. The column space is the space of all vectors that we could possibly reach for some $\\mathbf{x}$ with $\\mathbf{A}\\mathbf{x}$. The left nullspace is the space of all vectors that are impossible for us to reach using $\\mathbf{A}\\mathbf{x}$ for any $\\mathbf{x}$. At this point, we can begin to intuit where error from our linear approximation of $\\mathbf{y}$ will come from.  If there $\\mathbf{y}$ has components in the left null space of $\\mathbf{A}$, then it will be impossible for us to recreate $\\mathbf{y}$ exactly for any $\\mathbf{x}$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To express $\\mathbf{y}$ in terms of the column space and the left null space, we will use the SVD. $\\mathbf{A}$ can be decomposed via SVD as\n",
      "\n",
      "\\begin{align}\n",
      "\\mathbf{A} &= \\mathbf{U}\\Sigma\\mathbf{V}^T \\\\\n",
      " &= \\sum_{i=1}^{r}\\sigma_i\\mathbf{u}_i\\mathbf{v}_i^T \\\\\n",
      "\\end{align}\n",
      "\n",
      "where\n",
      " - $\\mathbf{U}\\in\\mathbb{R}^{m\\times m}$\n",
      " - $\\mathbf{V}\\in\\mathbb{R}^{n\\times n}$\n",
      " - $\\Sigma\\in\\mathbb{R}^{m\\times n}$\n",
      " \n",
      "and\n",
      " - the columns of $\\mathbf{U}$ (the _left singular vectors_) form an orthogonal basis for $\\mathbb{R}^m$,\n",
      " - the columns of $\\mathbf{V}$ (the _right singular vectors_) form an orthogonal basis for $\\mathbb{R}^n$,\n",
      " - $\\Sigma$ is diagonal with diagonal entries $\\sigma_i$, and the number of nonzero $\\sigma_i$ is $r$. \n",
      "\n",
      "Therefore\n",
      " - the right singular vectors corresponding to the zero entries of $\\Sigma$ form an orthogonal basis for the nullspace of $\\mathbf{A}$,\n",
      " - the right singular vectors corresponding to the nonzero entries of $\\Sigma$ form an orthogonal basis for the row space of $\\mathbf{A}$,\n",
      " - the left singular vectors corresponding to the zero entries of $\\Sigma$ form an orthogonal basis for the left nullspace of $\\mathbf{A}$.\n",
      " - the left singular vectors corresponding to the nonzero entries of $\\Sigma$ form an orthogonal basis for the column space of $\\mathbf{A}$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using the SVD, we can determine exactly how much error we can expect when approximating $\\mathbf{y}$ using $\\mathbf{A}\\mathbf{x}$. The error in our approximation will be exactly captured in the left null space of $\\mathbf{A}$. That is, we cannot possibly reach anywhere within the left nullspace using $\\mathbf{A}\\mathbf{x}$ for any $\\mathbf{x}\\in\\mathbb{R}^n$. Therefore, we should consider the projection of $\\mathbf{y}$ onto the columns of $\\mathbf{U}$ that form a basis for the left null space of $\\mathbf{A}$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This projection is expressed as \n",
      "\\begin{align}\n",
      "\\mathbf{y} &= \\mathbf{U}\\mathbf{U}^T\\mathbf{y} & \\mathbf{U}\\text{ is orthogonal, so }\\mathbf{U}\\mathbf{U}^T=\\mathbf{I} \\\\\n",
      " &= \\mathbf{U}(\\mathbf{U}_{\\sigma=0}^T\\mathbf{y}+\\mathbf{U}_{\\sigma\\neq0}^T\\mathbf{y}) \\\\\n",
      "\\end{align}\n",
      "\n",
      "where $\\mathbf{U}_{\\sigma=0}$ contains the left singular vectors corresponding to the zero entries of $\\Sigma$, and $\\mathbf{U}_{\\sigma\\neq0}$ contains the left singular vectors corresponding to the nonzero entries of $\\Sigma$. That is, $\\mathbf{U}_{\\sigma=0}$ forms a basis for the left null space and $\\mathbf{U}_{\\sigma\\neq0}$ forms a basis for the column space. Projecting $\\mathbf{y}$ onto $\\mathbf{U}_{\\sigma=0}$ and $\\mathbf{U}_{\\sigma\\neq0}$ expresses the components of $\\mathbf{y}$ as the combination of a left null space projection and a column space projection."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From least squares analysis, the least squares-optimal $\\mathbf{x}$ is\n",
      "\n",
      "$$\n",
      "\\hat{\\mathbf{x}}=(\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T\\mathbf{y}\n",
      "$$\n",
      "\n",
      "so \n",
      "\n",
      "$$\n",
      "\\mathbf{A}\\hat{\\mathbf{x}}=\\mathbf{A}(\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T\\mathbf{y}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Further,\n",
      "\n",
      "\\begin{align}\n",
      "\\mathbf{A}\\hat{\\mathbf{x}} &= \\mathbf{A}(\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T\\mathbf{y} \\\\\n",
      " &= U\\Sigma V^T(V\\Sigma U^TU\\Sigma V^T)^{-1}V\\Sigma U^T\\mathbf{y} \\\\\n",
      " &= U\\Sigma V^T(V\\Sigma^2 V^T)^{-1}V\\Sigma U^T\\mathbf{y} \\\\\n",
      " &= U\\Sigma V^TV\\Sigma^{-2} V^TV\\Sigma U^T\\mathbf{y} \\\\\n",
      " &= U\\Sigma \\Sigma^{-2} \\Sigma U^T\\mathbf{y} \\\\\n",
      "\\end{align}\n",
      "\n",
      "At this point, we need to be careful. Whan happens to $\\Sigma \\Sigma^{-2} \\Sigma$ in the next step depends on the rank and dimensions of $\\mathbf{A}$. The cases are\n",
      "\n",
      "| $A_{m\\times n}$ is    | full rank | Not full rank |\n",
      "|-------:|:---------:|:-------------:|\n",
      "|__square__,   $m=n$|    $I_{m\\times m}$       |    $I_{m\\times m,\\ r}$           |\n",
      "|__fat__,   $m<n$|    sdfs       |    sdfs           |\n",
      "|__skinny__,  $m>n$|    sdfs       |    sdfs           |\n",
      "\n",
      "In the cases that $\\mathbf{A}$ is square or fat and full rank, does\n",
      "\\begin{align}\n",
      "\\mathbf{A}\\hat{\\mathbf{x}} &= U\\Sigma \\Sigma^{-2} \\Sigma U^T\\mathbf{y} \\\\\n",
      " &= UU^T\\mathbf{y} \\\\\n",
      " &= \\mathbf{y}\\\\ \n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The $RMSE$ can then be expressed as \n",
      "\n",
      "\\begin{align}\n",
      "RMSE &= \\|\\mathbf{y}-\\mathbf{A}\\mathbf{x}\\|_2 \\\\\n",
      " &= \\|\\mathbf{U}\\mathbf{U}^T\\mathbf{y}-\\mathbf{U}\\Sigma\\mathbf{V}^T\\mathbf{x}\\|_2 \\\\\n",
      " &= \\|\\mathbf{U}(\\mathbf{U}_{\\sigma=0}^T\\mathbf{y}+\\mathbf{U}_{\\sigma\\neq0}^T\\mathbf{y})-\n",
      "     \\mathbf{U}\\Sigma\\mathbf{V}^T\\mathbf{x}\\|_2 \\\\\n",
      " &= \\|\\mathbf{U}(\\mathbf{U}_{\\sigma=0}^T\\mathbf{y}+\\mathbf{U}_{\\sigma\\neq0}^T\\mathbf{y}-\n",
      "     \\Sigma\\mathbf{V}^T\\mathbf{x})\\|_2 \\\\\n",
      " &= \\|\\mathbf{U}\\mathbf{U}_{\\sigma=0}^T\\mathbf{y}\\|_2 \\\\\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Appendix"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Separability of decoding dimensions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With no other constraints on $\\mathbf{x}$,\n",
      "$$\n",
      "\\begin{bmatrix}\n",
      "| & | & & | \\\\\n",
      "\\mathbf{y}_0 & \\mathbf{y}_1 & \\cdots & \\mathbf{y}_d \\\\\n",
      "| & | & & | \\\\\n",
      "\\end{bmatrix}\n",
      "=\n",
      "\\begin{bmatrix}\n",
      "| & | &  & | \\\\\n",
      "\\mathbf{a}_0 & \\mathbf{a}_1 & \\cdots & \\mathbf{a}_n \\\\\n",
      "| & | &  & | \n",
      "\\end{bmatrix}\n",
      "\\begin{bmatrix}\n",
      "| & | & & | \\\\\n",
      "\\mathbf{x}_0 & \\mathbf{x}_1 & \\cdots & \\mathbf{x}_d \\\\\n",
      "| & | & & | \\\\\n",
      "\\end{bmatrix}\n",
      "$$\n",
      "\n",
      "can be separated into\n",
      "\\begin{align}\n",
      "\\begin{bmatrix}\n",
      "    | \\\\\n",
      "    \\mathbf{y}_0 \\\\\n",
      "    | \\\\\n",
      "\\end{bmatrix}\n",
      "&=\n",
      "\\begin{bmatrix}\n",
      "    | & | &  & | \\\\\n",
      "    \\mathbf{a}_0 & \\mathbf{a}_1 & \\cdots & \\mathbf{a}_n \\\\\n",
      "    | & | &  & | \n",
      "\\end{bmatrix}\n",
      "\\begin{bmatrix}\n",
      "    | \\\\\n",
      "    \\mathbf{x}_0 \\\\\n",
      "    | \\\\\n",
      "\\end{bmatrix} \\\\ \\\\\n",
      "\\begin{bmatrix}\n",
      "    | \\\\\n",
      "    \\mathbf{y}_1 \\\\\n",
      "    | \\\\\n",
      "\\end{bmatrix}\n",
      "&=\n",
      "\\begin{bmatrix}\n",
      "    | & | &  & | \\\\\n",
      "    \\mathbf{a}_0 & \\mathbf{a}_1 & \\cdots & \\mathbf{a}_n \\\\\n",
      "    | & | &  & | \n",
      "\\end{bmatrix}\n",
      "\\begin{bmatrix}\n",
      "    | \\\\\n",
      "    \\mathbf{x}_1 \\\\\n",
      "    | \\\\\n",
      "\\end{bmatrix} \\\\\n",
      "&\\ \\vdots \\\\\n",
      "\\begin{bmatrix}\n",
      "    | \\\\\n",
      "    \\mathbf{y}_d \\\\\n",
      "    | \\\\\n",
      "\\end{bmatrix}\n",
      "&=\n",
      "\\begin{bmatrix}\n",
      "    | & | &  & | \\\\\n",
      "    \\mathbf{a}_0 & \\mathbf{a}_1 & \\cdots & \\mathbf{a}_n \\\\\n",
      "    | & | &  & | \n",
      "\\end{bmatrix}\n",
      "\\begin{bmatrix}\n",
      "    | \\\\\n",
      "    \\mathbf{x}_d \\\\\n",
      "    | \\\\\n",
      "\\end{bmatrix} \\\\ \\\\\n",
      "\\end{align}\n",
      "\n",
      "where we can consider each equation separately.\n",
      "\n",
      "Note that there are times when we do impose constraints on $\\mathbf{x}$. For example, when using the generalized form of least squares to find $\\mathbf{x}$, we seek\n",
      "\n",
      "$\\DeclareMathOperator*{\\argmin}{arg\\,min}$\n",
      "$$\n",
      "\\argmin_x \\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2^2+\\|\\mathbf{\\Gamma}\\mathbf{x}\\|_2^2\n",
      "$$\n",
      "\n",
      "Here, $\\mathbf{\\Gamma}$ could impose a relationship between the columns of $\\mathbf{x}$."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}